{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r05uUZzG-h64"
      },
      "outputs": [],
      "source": [
        "!curl https://ollama.ai/install.sh | sh"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pjHtU_QuKqvx"
      },
      "outputs": [],
      "source": [
        "!pip install colab-xterm #https://pypi.org/project/colab-xterm/\n",
        "%load_ext colabxterm\n",
        "%xterm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6DH09pKKb5P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a4d7c45-41ff-4a2c-d2f6-43eff95e68a7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m990.6/990.6 kB\u001b[0m \u001b[31m52.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m379.9/379.9 kB\u001b[0m \u001b[31m27.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.1/140.1 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.1/141.1 kB\u001b[0m \u001b[31m12.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-community -q\n",
        "# Import Ollama module from Langchain\n",
        "from langchain_community.llms import Ollama"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAptiR17ZU5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78aea16a-2413-4a2c-c5b4-b4fa97876ba4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.5/15.5 MB\u001b[0m \u001b[31m50.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m360.7/360.7 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m18.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install llama-index.core -q\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xFFkDJq1RarH"
      },
      "outputs": [],
      "source": [
        "# Initialize an instance of the Ollama model\n",
        "llm = Ollama(model=\"gemma2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "81fQMU-9E__C"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFXOMACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def cleanup_generated_text(text):\n",
        "    \"\"\" Function to remove unwanted conversations and repeated content from generated text. \"\"\"\n",
        "    unwanted_phrases = [\n",
        "        \"assistant\", \"You're welcome\", \"ha\", \"okay\", \"nice\", \"goodbye\", \"thank you\",\n",
        "        \"ahem\", \"we're\", \"I'll\", \"That's\", \"sounds\", \"agreed\", \"chatty\", \"wave\",\n",
        "        \"handshake\", \"hug\", \"laughter\", \"applause\", \"mic drop\", \"explosion\",\n",
        "        \"fireworks\", \"finale\", \"I'm done\", \"let's just stop\", \"ha\", \"same to you\",\n",
        "        \"mission accomplished\", \"excellent\", \"guilty as charged\", \"my secret's safe with you\",\n",
        "        \"under wraps\", \"fun little chat\", \"see you next time\", \"have a great day\",\n",
        "        \"virtual smile\", \"smile\", \"wrapped this up\"\n",
        "    ]\n",
        "    lines = text.split(\"\\n\")\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        if any(phrase.lower() in line.lower() for phrase in unwanted_phrases):\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "\n",
        "\n",
        "def save_results_to_txt(file_path, results):\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for chunk_id, result in enumerate(results):\n",
        "            file.write(f\"Chunk {chunk_id+1}:\\n{result}\\n{'-'*100}\\n\")\n",
        "    logging.info(f\"Results saved to {file_path}\")\n",
        "\n",
        "def extract_information(result):\n",
        "    import re\n",
        "\n",
        "    # Extract information from the formatted result\n",
        "    assets = []\n",
        "    locations = []\n",
        "    ownerships = []\n",
        "    commodities = []\n",
        "    statuses = []\n",
        "\n",
        "    # Regular expressions to match the patterns\n",
        "    asset_pattern = re.compile(r'physical assets:\\s*\\[([^\\]]+)\\]')\n",
        "    location_pattern = re.compile(r'locations:\\s*\\[([^\\]]+)\\]')\n",
        "    ownership_pattern = re.compile(r'ownerships:\\s*\\[([^\\]]+)\\]')\n",
        "    commodity_pattern = re.compile(r'commodities:\\s*\\[([^\\]]+)\\]')\n",
        "    status_pattern = re.compile(r'status:\\s*\\[([^\\]]+)\\]')\n",
        "\n",
        "    # Search for the patterns in the result\n",
        "    asset_match = asset_pattern.search(result)\n",
        "    location_match = location_pattern.search(result)\n",
        "    ownership_match = ownership_pattern.search(result)\n",
        "    commodity_match = commodity_pattern.search(result)\n",
        "    status_match = status_pattern.search(result)\n",
        "\n",
        "    if asset_match:\n",
        "        assets = [asset.strip() for asset in asset_match.group(1).split(',')]\n",
        "    if location_match:\n",
        "        locations = [location.strip() for location in location_match.group(1).split(',')]\n",
        "    if ownership_match:\n",
        "        ownerships = [ownership.strip() for ownership in ownership_match.group(1).split(',')]\n",
        "    if commodity_match:\n",
        "        commodities = [commodity.strip() for commodity in commodity_match.group(1).split(',')]\n",
        "    if status_match:\n",
        "        statuses = [status.strip() for status in status_match.group(1).split(',')]\n",
        "\n",
        "    return assets, locations, ownerships, commodities, statuses\n",
        "\n",
        "def list_tables(db_path):\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT name FROM sqlite_master WHERE type='table';\")\n",
        "            tables = cursor.fetchall()\n",
        "            return [table[0] for table in tables]\n",
        "    except sqlite3.Error as e:\n",
        "        logging.error(f\"Database error: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_document_texts(db_path):\n",
        "    try:\n",
        "        with sqlite3.connect(db_path) as conn:\n",
        "            cursor = conn.cursor()\n",
        "            cursor.execute(\"SELECT document_text FROM filings\")\n",
        "            rows = cursor.fetchall()\n",
        "            return [row[0] for row in rows]\n",
        "    except sqlite3.Error as e:\n",
        "        logging.error(f\"Database error: {e}\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error: {e}\")\n",
        "    return []\n",
        "\n",
        "def process_document_chunks(db_path):\n",
        "    document_texts = extract_document_texts(db_path)\n",
        "\n",
        "    if not document_texts:\n",
        "        logging.info(f\"No documents found in the 'filings' table.\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    chunk_size = 1024  # Increased chunk size with overlap\n",
        "    chunk_overlap = 20\n",
        "    node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    for document_text in tqdm(document_texts, desc=\"Processing documents\"):\n",
        "        nodes = node_parser.get_nodes_from_documents([Document(text=document_text)], show_progress=False)\n",
        "\n",
        "        for chunk_id, node in tqdm(enumerate(nodes), total=len(nodes), desc=\"Processing chunks\"):  # Process all chunks\n",
        "            chunk = node.text\n",
        "            result = query_chunk(chunk)\n",
        "            results.append((chunk_id, result))\n",
        "            logging.info(f\"Processed chunk {chunk_id}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_HL_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/HL_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6KCWHcVQXrRY"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/HL_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/HL_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/HL_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_HL_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FcVADjQh1PoA"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_HL_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# List of countries fetched from Our World in Data\n",
        "countries_list = [\n",
        "    \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"Andorra\", \"United Arab Emirates\",\n",
        "    \"Argentina\", \"Armenia\", \"American Samoa\", \"Antigua and Barbuda\", \"Australia\", \"Austria\",\n",
        "    \"Azerbaijan\", \"Burundi\", \"Belgium\", \"Benin\", \"Burkina Faso\", \"Bangladesh\", \"Bulgaria\",\n",
        "    \"Bahrain\", \"Bahamas\", \"Bosnia and Herzegovina\", \"Saint Barthelemy\", \"Belarus\", \"Belize\",\n",
        "    \"Bermuda\", \"Bolivia\", \"Brazil\", \"Barbados\", \"Brunei\", \"Bhutan\", \"Botswana\",\n",
        "    \"Central African Republic\", \"Canada\", \"Switzerland\", \"Chile\", \"China\", \"Cote d'Ivoire\",\n",
        "    \"Cameroon\", \"Democratic Republic of Congo\", \"Congo\", \"Colombia\", \"Comoros\", \"Cape Verde\",\n",
        "    \"Costa Rica\", \"Cuba\", \"Christmas Island\", \"Cayman Islands\", \"Cyprus\", \"Czechia\", \"Germany\",\n",
        "    \"Djibouti\", \"Dominica\", \"Denmark\", \"Dominican Republic\", \"Algeria\", \"Ecuador\", \"Egypt\",\n",
        "    \"Eritrea\", \"Spain\", \"Estonia\", \"Ethiopia\", \"Finland\", \"Fiji\", \"Falkland Islands\", \"France\",\n",
        "    \"Faroe Islands\", \"Micronesia (country)\", \"Gabon\", \"United Kingdom\", \"Georgia\", \"Ghana\",\n",
        "    \"Gibraltar\", \"Guinea\", \"Gambia\", \"Guinea-Bissau\", \"Equatorial Guinea\", \"Greece\", \"Grenada\",\n",
        "    \"Greenland\", \"Guatemala\", \"French Guiana\", \"Guam\", \"Guyana\", \"Hong Kong\", \"Honduras\",\n",
        "    \"Croatia\", \"Haiti\", \"Hungary\", \"Indonesia\", \"Isle of Man\", \"India\", \"Ireland\", \"Iran\",\n",
        "    \"Iraq\", \"Iceland\", \"Israel\", \"Italy\", \"Jamaica\", \"Jersey\", \"Jordan\", \"Japan\", \"Kazakhstan\",\n",
        "    \"Kenya\", \"Kyrgyzstan\", \"Cambodia\", \"Kiribati\", \"Saint Kitts and Nevis\", \"South Korea\",\n",
        "    \"Kuwait\", \"Laos\", \"Lebanon\", \"Liberia\", \"Libya\", \"Saint Lucia\", \"Liechtenstein\", \"Sri Lanka\",\n",
        "    \"Lesotho\", \"Lithuania\", \"Luxembourg\", \"Latvia\", \"Macao\", \"Morocco\", \"Monaco\", \"Moldova\",\n",
        "    \"Madagascar\", \"Maldives\", \"Mexico\", \"Marshall Islands\", \"North Macedonia\", \"Mali\", \"Malta\",\n",
        "    \"Myanmar\", \"Montenegro\", \"Mongolia\", \"Northern Mariana Islands\", \"Mozambique\", \"Mauritania\",\n",
        "    \"Martinique\", \"Mauritius\", \"Malawi\", \"Malaysia\", \"Mayotte\", \"Namibia\", \"New Caledonia\",\n",
        "    \"Niger\", \"Norfolk Island\", \"Nigeria\", \"Nicaragua\", \"Niue\", \"Netherlands\", \"Norway\", \"Nepal\",\n",
        "    \"Nauru\", \"New Zealand\", \"Oman\", \"Kosovo\", \"Pakistan\", \"Panama\", \"Pitcairn\", \"Peru\",\n",
        "    \"Philippines\", \"Palau\", \"Papua New Guinea\", \"Poland\", \"Puerto Rico\", \"North Korea\",\n",
        "    \"Portugal\", \"Paraguay\", \"Palestine\", \"French Polynesia\", \"Qatar\", \"Reunion\", \"Romania\",\n",
        "    \"Russia\", \"Rwanda\", \"Saudi Arabia\", \"Sudan\", \"Senegal\", \"Singapore\", \"Solomon Islands\",\n",
        "    \"Sierra Leone\", \"El Salvador\", \"San Marino\", \"Somalia\", \"Saint Pierre and Miquelon\",\n",
        "    \"Serbia\", \"South Sudan\", \"Sao Tome and Principe\", \"Suriname\", \"Slovakia\", \"Slovenia\",\n",
        "    \"Sweden\", \"Eswatini\", \"Seychelles\", \"Syria\", \"Turks and Caicos Islands\", \"Chad\", \"Togo\",\n",
        "    \"Thailand\", \"Tajikistan\", \"Tokelau\", \"Turkmenistan\", \"East Timor\", \"Tonga\", \"Trinidad and Tobago\",\n",
        "    \"Tunisia\", \"Turkey\", \"Tuvalu\", \"Taiwan\", \"Tanzania\", \"Uganda\", \"Ukraine\", \"Uruguay\",\n",
        "    \"Uzbekistan\", \"Vatican\", \"Saint Vincent and the Grenadines\", \"Venezuela\", \"British Virgin Islands\",\n",
        "    \"United States Virgin Islands\", \"USA\", \"Vietnam\", \"Vanuatu\", \"Samoa\", \"Yemen\", \"South Africa\",\n",
        "    \"Zambia\", \"Zimbabwe\"\n",
        "]\n",
        "\n",
        "# Function to replace specified entities with 'Hecla Mining Company' and remove duplicates\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        'Company', 'company', 'The company', 'The Company', 'The company owns 100%', '100.00 %', '100.00%', 'Hecla Limited', 'Hecla', 'the company', 'Company owned', 'Hecla Mining Company and its subsidiaries', 'The company (implied)', 'Company owning the mines'\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    # Replace aliases with 'Hecla Mining Company'\n",
        "    cleaned_entities = ['Hecla Mining Company' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_HL_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j6IGz4k21PqV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_HL_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"Hecla Mining Company\" should not be separated by extra commas..\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - In the 'location' column, locations can be replaced by their shorter version. As an example, 'Republic of Washington' can be replaced by 'Washington'.\n",
        "   - Any variations of the company name and ownership in the 'ownership' column should be changed to 'Hecla Mining Company'. 'Hecla Quebec Inc.', 'Hecla Montana' and private or USFS or BLM administered land should remain unchanged.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - Ensure that there are no repetitions or redundant entries in all the cells.\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_HL_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vENeRhLL1Pu3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        " #Load the CSV file\n",
        "file_path = '/content/LLMcleaned_HL_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_HL_10K_test_wcountries.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_DvvHXvv1PxK"
      },
      "outputs": [],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_HL_10K_wcountries_nostatus.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\n",
        "Here are some example mappings:\n",
        "- Alaska -> United States of America\n",
        "- Casa Berardi -> Canada\n",
        "- British Columbia -> Canada\n",
        "- Idaho -> United States of America\n",
        "- New York -> United States of America\n",
        "- Keno Hill -> Canada\n",
        "- Greens Creek -> United States of America\n",
        "- San Sebastian -> Mexico\n",
        "- Yukon -> Canada\n",
        "- Lucky Friday -> United States of America\n",
        "- Admiralty Island -> United States of America\n",
        "- Montana -> United States of America\n",
        "\n",
        "If USA is the country that a location is situated in, it should be returned as \"United States of America\".\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "df['Countries'] = df['location'].apply(extract_countries)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted_llm_nostatus.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCW-0REh1Pzg"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/countries_extracted_llm_nostatus.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to clean the 'location' column based on 'Countries'\n",
        "def clean_location_based_on_countries(row):\n",
        "    if pd.isna(row['Countries']) or pd.isna(row['location']):\n",
        "        return row['location']\n",
        "\n",
        "    countries = row['Countries'].split(', ')\n",
        "    location = row['location']\n",
        "\n",
        "    # Variants of United States of America\n",
        "    usa_variants = ['United States of America', 'USA', 'US', 'United States']\n",
        "\n",
        "    # Remove countries and their variants from the location column\n",
        "    for country in countries:\n",
        "        if country in location:\n",
        "            location = location.replace(country, '')\n",
        "        if country in usa_variants:\n",
        "            for variant in usa_variants:\n",
        "                if variant in location:\n",
        "                    location = location.replace(variant, '')\n",
        "\n",
        "    # Remove any extra commas and whitespace\n",
        "    location = ', '.join(filter(None, [loc.strip() for loc in location.split(',')]))\n",
        "\n",
        "    return location\n",
        "\n",
        "# Apply the function to each row\n",
        "data['location'] = data.apply(clean_location_based_on_countries, axis=1)\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/countries_from_locations.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "output_file_path\n",
        "print('Countries have been removed from locations')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/countries_from_locations.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# List of US states\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n",
        "    \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
        "    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n",
        "    \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
        "    \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\",\n",
        "    \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/HL_w_states.CSV'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "Ot3gLKLHaWGA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# AA database creation"
      ],
      "metadata": {
        "id": "c-Lub6Pw8ZZ4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFByUirr1P1n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_AA_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/AA_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ih-gOl1M1P3w"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/AA_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/AA_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/AA_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_AA_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_AA_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to replace specified entities with company name and remove duplicates\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Company\", \"company\", \"The company\", \"The Company\", \"Company's\", \"we\", \"Hecla Limited\", \"Alcoa Corporation subsidiaries\", \"Alcoa-operated\", \"Alcoa personnel\", \"Alcoa\", \"Alcoa personnel\", \"the company\", \"Company owned\", \"Company's\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    # Replace aliases with 'Hecla Mining Company'\n",
        "    cleaned_entities = ['Alcoa Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_AA_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "RpvfOf2MIa9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_AA_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"Hecla Mining Company\" should not be separated by extra commas..\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - In the 'location' column, locations can be replaced by their shorter version. As an example, 'Republic of Washington' can be replaced by 'Washington'.\n",
        "   - Any variations of the company name (Alcoa Corporation) should be changed to 'Alcoa Corporation'.\n",
        "   - The percentage ownership of a company should be given in brackets along with the company with the percentage ownership. If the percentage ownership is given without an accompanying company, the percentages should be removed. For example, if 'Alcoa Corporation (25.1%)' is given, it should remain unchanged. But if '42.18 %, 34.97 %' is given, it should be removed.\n",
        "   - 'Alcoa Trasformazioni S.r.l.', 'Alumina', 'PARTER, 'Ma'aden', 'Estreito Energia S.A.','Rio Tinto' and are other companies that are not Alcoa should not be changed.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - Ensure that there are no repetitions or redundant entries in all the cells.\n",
        "   -\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/3LLMcleaned_AA_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "uuV7XQbSMjOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/3LLMcleaned_AA_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\n",
        "Here are some example mappings:\n",
        "- Alaska -> United States of America\n",
        "- San Ciprián -> Puerto Rico\n",
        "- Juruti -> Brazil\n",
        "- Darling Range -> Australia\n",
        "- Kwinana -> Australia\n",
        "- Para State -> Brazil\n",
        "- Avila -> Spain\n",
        "- A Coruña -> Spain\n",
        "- Pocos De Caldas -> Brazil\n",
        "- Maryland -> USA\n",
        "- Arkansas -> USA\n",
        "- Indiana -> USA\n",
        "- Texas -> USA\n",
        "- Suriname -> Suriname\n",
        "- Pinjarra -> Australia\n",
        "- MosjÃ¸en -> Norway\n",
        "- Massena -> USA\n",
        "- Trombetas -> Brazil\n",
        "- Baie-Comeau -> Canada\n",
        "- Huntly -> Australia\n",
        "- Willowdale -> Australia\n",
        "- St. Croix -> St. Croix\n",
        "\"\"\"\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "df['Countries'] = df['location'].apply(extract_countries)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")\n"
      ],
      "metadata": {
        "id": "E0nDIRa4mtD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'  # Update this path to the location of file\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/AA_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "507CiXyEZbxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEM extraction"
      ],
      "metadata": {
        "id": "Op_RUDy-BlH2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_NEM_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/NEM_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "rcdpPGVGAi2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/NEM_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/NEM_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/NEM_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_NEM_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "hIs6yoAbOctd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_NEM_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to replace specified entities with company names and remove duplicates\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Company\", \"company\", \"The company\", \"the company\", \"The Company\", \"Company's\", \"the Company\", \"we\", \"NEWMONT CORPORATION\", \"Newmont's ownership or economic interest\", \"Company owns or controls land\", \"Newmont\", \"Newmont (majority)\", \"Newmont Corporation (formerly)\", \"100% owned by the Company\", \"Newmont Stockholders\", \"100% owned by the Company\", \"100% by Newmont\", \"Newmont's ownership or economic interest\", \"the company\", \"Company owned\", \"Company's\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Newmont Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_NEM_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "TPe60ai0qnIM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_NEM_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"Newmont Corporation\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - Any variations of the company name (Newmont Corporation) should be changed to 'Newmont Corporation'.\n",
        "   - Remove percentages in the 'ownership' column.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/3LLMcleaned_NEM_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "-0ORVEm1qwz6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/3LLMcleaned_NEM_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\n",
        "Here are some example mappings:\n",
        "- Penasquito -> Mexico\n",
        "- Lihir -> Papua New Guinea\n",
        "- Red Chris -> Canada\n",
        "- Boddington -> Australia\n",
        "- Norte Abierto -> Chile\n",
        "- Cerro Negro -> Brazil\n",
        "- Pueblo Viejo -> Dominican Republic\n",
        "- Cripple Creek Mine -> USA\n",
        "- Turquoise Ridge -> USA\n",
        "- Carlin -> USA\n",
        "- Telfer -> Australia\n",
        "- Porcupine -> USA\n",
        "- Morobe Province -> Papua New Guinea\n",
        "- San Pedro De Macoris -> Dominican Republic\n",
        "\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "df['Countries'] = df['location'].apply(extract_countries)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")\n"
      ],
      "metadata": {
        "id": "2_jX79HNCHCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/NEM_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "5u53QNNZZ0pk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are 'Deposits/Districts' from the table (Boddington Open Pit, Porcupine Open Pit and Total Pueblo Viejo are examples of physical assets in the text).\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/NEM_table.txt'\n",
        "output_txt_path = '/content/NEM_tables_output.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")"
      ],
      "metadata": {
        "id": "m_vWHYuzIMun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SCCO"
      ],
      "metadata": {
        "id": "AbIwkGeChE52"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_SCCO_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/SCCO_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "NPbQuswrd1OU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/SCCO_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/SCCO_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/SCCO_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_SCCO_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aFNy3MeCh2AJ",
        "outputId": "5d856871-979a-4c0a-817e-a82c40916a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_SCCO_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Southern Copper\", \"Company\", \"Company (implied)\", \"company\", \"The company\", \"SCC\", \"the company\", \"We\", \"owned by the company\", \"Company's\", \"Leased by the Company\", \"company (not specified)\", \"owned by the company mentioned in the text\", \"Company (co-financed)\", \"company (not specified)\", \"The Company\", \"Company's\", \"the Company\", \"we\", \"Company owns or controls land\", \"100% owned by the Company\", \"100% owned by the Company\", \"the company\", \"Company owned\", \"Company's\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Southern Copper Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_SCCO_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "kkz-syB3BQBd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_SCCO_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"Southern Copper Corporation\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - Remove percentages in the 'ownership' column.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_SCCO_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRCCvsUDEMwH",
        "outputId": "92b22ff7-ffdf-4507-9d04-10d025ba7606"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [10:23<00:00, 103.99s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api -q\n"
      ],
      "metadata": {
        "id": "MYfZhVxtGf9r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_SCCO_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "IoFn6edUGxr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'  # Update this path to the location of your file\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/SCCO_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "VniG0o77Z9xB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are all the facility names and property names (Cuajone open-put mine, Pilares and Tia Maria are physical assets in the text).\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/SCCO_table.txt'\n",
        "output_txt_path = '/content/SCCO_tables_output_4.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "AUEYtVBqMzF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/SCCO_tables_output.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/SCCO_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/SCCO_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/SCCO_tables_output.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "DHs8JRw4L83J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#FCX"
      ],
      "metadata": {
        "id": "RQ_EE_h0MuAI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_FCX_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/FCX_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "GFGN5oYVHaTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/FCX_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/FCX_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FCX_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_FCX_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "aIzVGPUjOd3j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_FCX_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Company\", \"Company (implied)\", \"FCX (100%)\", \"Company (not specified)\", \"company\", \"The company\", \"FCX\", \"the company\", \"FMC\", \"Freeport Minerals Corporation\", \"We\", \"owned by the company\", \"Company owns facilities\", \"Company's\", \"Freeport Minerals Corporation (FMC)\", \"FCX owns 100%\", \"Leased by the Company\", \"company (not specified)\", \"wholly owned\", \"wholly owned by the company\", \"FCX - 100% basis\", \"FCX (100%)\",  \"Company (co-financed)\", \"Partially owned by the company\", \"owned and operated by the company\", \"FCX affiliates\", \"operated by FCX\", \"company (not specified)\", \"The Company\", \"Company's\", \"the Company\", \"we\", \"Company owns or controls land\", \"100% owned by the Company\", \"100% owned by the Company\", \"the company\", \"Company owned\", \"Company's\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Freeport-McMoran' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_FCX_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "_Fqjfu1dkavt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_FCX_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"Freeport-McMoran\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - Remove percentages in the 'ownership' column.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - Any variation of 'FCX' or 'Freeport Mc-Moran' or 'company' should be replaced with 'Freeport-McMoran'.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/2LLMcleaned_FCX_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "9sjDkUIa36qV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api -q\n"
      ],
      "metadata": {
        "id": "IxRGS6nrKYjJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/2LLMcleaned_FCX_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "vXgQPUjZ6-SH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/FCX_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "SCPZYFXlaHjs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "def process_data(input_file, output_file):\n",
        "    # Read the input file into a DataFrame\n",
        "    df = pd.read_csv(input_file)\n",
        "\n",
        "    # Create a new DataFrame to store the rows to be addedD\n",
        "    new_rows = []\n",
        "\n",
        "    # Iterate through the DataFrame\n",
        "    for index, row in df.iterrows():\n",
        "        location = row['location']\n",
        "\n",
        "        # Check if location is a string and contains 'mine'\n",
        "        if isinstance(location, str) and 'mine' in location.lower():\n",
        "            # Create a new row with the 'location' and 'physical asset' set to the mine name\n",
        "            new_row = row.copy()\n",
        "            new_row['location'] = location\n",
        "            new_row['physical asset'] = location\n",
        "            new_rows.append(new_row)\n",
        "\n",
        "    # Convert new rows to a DataFrame\n",
        "    new_rows_df = pd.DataFrame(new_rows)\n",
        "\n",
        "    # Append the new rows to the original DataFrame\n",
        "    result_df = pd.concat([df, new_rows_df], ignore_index=True)\n",
        "\n",
        "    # Save the result to the output file\n",
        "    result_df.to_csv(output_file, index=False)\n",
        "# Example usage\n",
        "input_file = '/content/FCX_FINAL.csv'\n",
        "output_file = '/content/FCX_rowsplit.csv'\n",
        "process_data(input_file, output_file)\n"
      ],
      "metadata": {
        "id": "37gYgypdq7Zx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#XOM"
      ],
      "metadata": {
        "id": "UU1Xtz9GVxiV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "def process_document_chunks(db_path):\n",
        "    document_texts = extract_document_texts(db_path)\n",
        "\n",
        "    if not document_texts:\n",
        "        return []\n",
        "\n",
        "    chunk_counts = []\n",
        "    chunk_size = 1024  # Define the chunk size and overlap as before\n",
        "    chunk_overlap = 20\n",
        "    node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    for document_text in document_texts[:3]:  # Only consider the first 3 documents\n",
        "        nodes = node_parser.get_nodes_from_documents([Document(text=document_text)], show_progress=False)\n",
        "        chunk_counts.append(len(nodes))\n",
        "\n",
        "    return chunk_counts\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_XOM_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    pass  # No logging or error handling needed for this task\n",
        "else:\n",
        "    tables = list_tables(db_path)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and get the count of chunks for each document\n",
        "        chunk_counts = process_document_chunks(db_path)\n",
        "\n",
        "        # Output only the number of chunks for each document\n",
        "        for i, count in enumerate(chunk_counts):\n",
        "            print(f\"Document {i + 1} has {count} chunks.\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n"
      ],
      "metadata": {
        "id": "PbO-KAlxLiwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "def process_document_chunks(db_path):\n",
        "    document_texts = extract_document_texts(db_path)\n",
        "\n",
        "    if not document_texts:\n",
        "        logging.info(f\"No documents found in the 'filings' table.\")\n",
        "        return []\n",
        "\n",
        "    results = []\n",
        "    chunk_size = 1024  # Increased chunk size with overlap\n",
        "    chunk_overlap = 20\n",
        "    node_parser = SentenceSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "    for document_text in tqdm(document_texts, desc=\"Processing documents\"):\n",
        "        nodes = node_parser.get_nodes_from_documents([Document(text=document_text)], show_progress=False)\n",
        "\n",
        "        for chunk_id, node in tqdm(enumerate(nodes), total=len(nodes), desc=\"Processing chunks\"):  # Process all chunks\n",
        "            chunk = node.text\n",
        "            result = query_chunk(chunk)\n",
        "            results.append((chunk_id, result))\n",
        "            logging.info(f\"Processed chunk {chunk_id}\")\n",
        "\n",
        "    return results\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_XOM_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/XOM_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "NlygMrxETrvs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## parsing table"
      ],
      "metadata": {
        "id": "nkzHVc6wTekF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns in a location and uses in the production of goods and services. Examples of physical assets are refineries and refining capacities. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/XOM_tables_2.txt'\n",
        "output_txt_path = '/content/XOM_tables_output_2.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "RTjvS4qcVxAT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/XOM_tables_output_2.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/XOM_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/XOM_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/XOM_tables_output.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "Y_CtJmmNWlqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/XOM_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/XOM_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/XOM_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_XOM_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "jMo7ib_WW5qZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_XOM_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "       \"ExxonMobil Corporation\",  \"Corporation\", \"The Corporation\", \"Corporation and its consolidated affiliates\", \"ExxonMobil Oil Corporation (EMOC)\", \"ExxonMobil and its consolidated affiliates\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['ExxonMobil' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_XOM_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "_DqIVTldDYoy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_XOM_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities should be separated by a comma.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_XOM_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "kYbt4WHRHNme"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_XOM_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "1f5iS9MULb9O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/XOM_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "rijA7aZ9amk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OXY"
      ],
      "metadata": {
        "id": "OHDBIm48T6xj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_OXY_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/OXY_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "ZiQUx-N5QlVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/OXY_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/OXY_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/OXY_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_OXY_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "FP6nrtzrUa9v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_OXY_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Function to replace specified entities with 'Hecla Mining Company' and remove duplicates\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Occidental\", \"Corporation\", \"Occidental Petroleum Corporation and Subsidiaries\", \"Occidental subsidiaries\", \"OXY\", \"Occidental and its subsidiaries\", \"Occidental or its subsidiaries\",  \"partly owned by Occidental\", \"Occidental subsidiaries\", \"The Corporation\", \"Corporation and its consolidated affiliates\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    # Replace aliases with 'Hecla Mining Company'\n",
        "    cleaned_entities = ['Occidental Petroleum Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_OXY_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "fHAJgq_3cjJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_OXY_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities should be separated by a comma.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_OXY_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U0_jDxbCeN_9",
        "outputId": "783f7da2-d6f6-423a-ea09-204084b6646a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [06:07<00:00, 61.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api -q!\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_OXY_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "HX2p41y12sAY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/OXY_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "oAA_9__c5Ujq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#CVX"
      ],
      "metadata": {
        "id": "2D8ziuDHKWNA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_CVX_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/CVX_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "LIGb5KM483jW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/CVX_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/CVX_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/CVX_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_CVX_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "PRgM_0MYKydq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_CVX_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Chevron\", \"Corporation\", \"parent\", \"company\", \"The Corporation\", \"Corporation and its consolidated affiliates\", \"The Company\", \"Company\", \"Company and its subsidiaries\", \"parent\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Chevron Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_CVX_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "Cv673WLkUCFy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_CVX_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities should be separated by a comma.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_CVX_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "85wLEITnjJJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_CVX_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "RWo9TYwQ_Utt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/CVX_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "7DOs3f8mbB0J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CVX parsing table"
      ],
      "metadata": {
        "id": "8bEC-kx2UiOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns in a location and uses in the production of goods and services. Examples of physical assets are refining operations.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/CVX_table.txt'\n",
        "output_txt_path = '/content/CVX_tables_output.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "LFeXzxndUkrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/CVX_tables_output.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/CVX_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/SCCO_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/CVX_tables_output.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "_at-Qp0kRxtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/CVX_tables_output.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/CVX_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/CVX_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/CVX_tables_output_final.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mh0K3tr_41aA",
        "outputId": "9bdd31f0-bb2b-458a-d6e7-c74db50a7ef0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#COP"
      ],
      "metadata": {
        "id": "-1-0tAWzEMql"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_COP_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/COP_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "nBBalN0SBJ_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/COP_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/COP_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/COP_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_COP_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dg6jKZUHEgL5",
        "outputId": "6db29705-9e32-4a3d-cfa3-9be4e11a929a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_COP_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Corporation and its consolidated affiliates\", \"The Company\", \"Company\", \"Company and its subsidiaries\", \"parent\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Chevron Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_COP_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "ViI0pWUkODNc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_COP_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities should be separated by a comma.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_COP_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-dPmpmOuYTL8",
        "outputId": "33dc1b4e-e438-46c9-ffc5-498d378a87cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [08:52<00:00, 88.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wikipedia-api -q"
      ],
      "metadata": {
        "id": "kpeo7JTCaCUP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_COP_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "id": "nrmVt1rWYbyU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/COP_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "mxTHmD0AbK2w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#MPC extraction"
      ],
      "metadata": {
        "id": "mKh3PI9Mi3sG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_MPC_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/MPC_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "r2R8Ibk2i50C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/MPC_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/MPC_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/MPC_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_MPC_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oKIWHfqWi-pB",
        "outputId": "6b1af52a-7279-429b-d507-67692bfb9621"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_MPC_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# List of countries fetched from Our World in Data\n",
        "countries_list = [\n",
        "    \"Aruba\", \"Afghanistan\", \"Angola\", \"Anguilla\", \"Albania\", \"Andorra\", \"United Arab Emirates\",\n",
        "    \"Argentina\", \"Armenia\", \"American Samoa\", \"Antigua and Barbuda\", \"Australia\", \"Austria\",\n",
        "    \"Azerbaijan\", \"Burundi\", \"Belgium\", \"Benin\", \"Burkina Faso\", \"Bangladesh\", \"Bulgaria\",\n",
        "    \"Bahrain\", \"Bahamas\", \"Bosnia and Herzegovina\", \"Saint Barthelemy\", \"Belarus\", \"Belize\",\n",
        "    \"Bermuda\", \"Bolivia\", \"Brazil\", \"Barbados\", \"Brunei\", \"Bhutan\", \"Botswana\",\n",
        "    \"Central African Republic\", \"Canada\", \"Switzerland\", \"Chile\", \"China\", \"Cote d'Ivoire\",\n",
        "    \"Cameroon\", \"Democratic Republic of Congo\", \"Congo\", \"Colombia\", \"Comoros\", \"Cape Verde\",\n",
        "    \"Costa Rica\", \"Cuba\", \"Christmas Island\", \"Cayman Islands\", \"Cyprus\", \"Czechia\", \"Germany\",\n",
        "    \"Djibouti\", \"Dominica\", \"Denmark\", \"Dominican Republic\", \"Algeria\", \"Ecuador\", \"Egypt\",\n",
        "    \"Eritrea\", \"Spain\", \"Estonia\", \"Ethiopia\", \"Finland\", \"Fiji\", \"Falkland Islands\", \"France\",\n",
        "    \"Faroe Islands\", \"Micronesia (country)\", \"Gabon\", \"United Kingdom\", \"Georgia\", \"Ghana\",\n",
        "    \"Gibraltar\", \"Guinea\", \"Gambia\", \"Guinea-Bissau\", \"Equatorial Guinea\", \"Greece\", \"Grenada\",\n",
        "    \"Greenland\", \"Guatemala\", \"French Guiana\", \"Guam\", \"Guyana\", \"Hong Kong\", \"Honduras\",\n",
        "    \"Croatia\", \"Haiti\", \"Hungary\", \"Indonesia\", \"Isle of Man\", \"India\", \"Ireland\", \"Iran\",\n",
        "    \"Iraq\", \"Iceland\", \"Israel\", \"Italy\", \"Jamaica\", \"Jersey\", \"Jordan\", \"Japan\", \"Kazakhstan\",\n",
        "    \"Kenya\", \"Kyrgyzstan\", \"Cambodia\", \"Kiribati\", \"Saint Kitts and Nevis\", \"South Korea\",\n",
        "    \"Kuwait\", \"Laos\", \"Lebanon\", \"Liberia\", \"Libya\", \"Saint Lucia\", \"Liechtenstein\", \"Sri Lanka\",\n",
        "    \"Lesotho\", \"Lithuania\", \"Luxembourg\", \"Latvia\", \"Macao\", \"Morocco\", \"Monaco\", \"Moldova\",\n",
        "    \"Madagascar\", \"Maldives\", \"Mexico\", \"Marshall Islands\", \"North Macedonia\", \"Mali\", \"Malta\",\n",
        "    \"Myanmar\", \"Montenegro\", \"Mongolia\", \"Northern Mariana Islands\", \"Mozambique\", \"Mauritania\",\n",
        "    \"Martinique\", \"Mauritius\", \"Malawi\", \"Malaysia\", \"Mayotte\", \"Namibia\", \"New Caledonia\",\n",
        "    \"Niger\", \"Norfolk Island\", \"Nigeria\", \"Nicaragua\", \"Niue\", \"Netherlands\", \"Norway\", \"Nepal\",\n",
        "    \"Nauru\", \"New Zealand\", \"Oman\", \"Kosovo\", \"Pakistan\", \"Panama\", \"Pitcairn\", \"Peru\",\n",
        "    \"Philippines\", \"Palau\", \"Papua New Guinea\", \"Poland\", \"Puerto Rico\", \"North Korea\",\n",
        "    \"Portugal\", \"Paraguay\", \"Palestine\", \"French Polynesia\", \"Qatar\", \"Reunion\", \"Romania\",\n",
        "    \"Russia\", \"Rwanda\", \"Saudi Arabia\", \"Sudan\", \"Senegal\", \"Singapore\", \"Solomon Islands\",\n",
        "    \"Sierra Leone\", \"El Salvador\", \"San Marino\", \"Somalia\", \"Saint Pierre and Miquelon\",\n",
        "    \"Serbia\", \"South Sudan\", \"Sao Tome and Principe\", \"Suriname\", \"Slovakia\", \"Slovenia\",\n",
        "    \"Sweden\", \"Eswatini\", \"Seychelles\", \"Syria\", \"Turks and Caicos Islands\", \"Chad\", \"Togo\",\n",
        "    \"Thailand\", \"Tajikistan\", \"Tokelau\", \"Turkmenistan\", \"East Timor\", \"Tonga\", \"Trinidad and Tobago\",\n",
        "    \"Tunisia\", \"Turkey\", \"Tuvalu\", \"Taiwan\", \"Tanzania\", \"Uganda\", \"Ukraine\", \"Uruguay\",\n",
        "    \"Uzbekistan\", \"Vatican\", \"Saint Vincent and the Grenadines\", \"Venezuela\", \"British Virgin Islands\",\n",
        "    \"United States Virgin Islands\", \"USA\", \"Vietnam\", \"Vanuatu\", \"Samoa\", \"Yemen\", \"South Africa\",\n",
        "    \"Zambia\", \"Zimbabwe\"\n",
        "]\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"MPC\", \"owned by the company\", \"Owned by MPC\", \"Marathon Petroleum Corporation (MPC)\", \"MPC through its ownership of MPLX LP\", \"Company\", \"Company-owned\", \"our\", \"company (implied)\", \"Marathon Petroleum Corporation (â€œMPCâ€)\"\n",
        "\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Marathon Petroleum Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Function to extract countries from location\n",
        "def extract_country(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    extracted_countries = [country for country in countries_list if country in location]\n",
        "    return ', '.join(extracted_countries) if extracted_countries else None\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_MPC_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "-SM4bcRywlhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_MPC_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries must be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - All individual organisations in the 'ownership' column should be separated by a comma.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities in the 'commodity' column should be separated by a comma and must be in title case.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - If there are any entries in the ownership column that are not organisations or companies, put N/A in the corresponding cell.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_MPC_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WegXJAT0WiY",
        "outputId": "8e256dc0-4832-41ef-873d-db1ef32b6644"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [04:47<00:00, 47.87s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\n",
        "If the 'location' column contains a state in USA, this should also be given in the 'Countries' column\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries&states_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTtJuYiU0h5h",
        "outputId": "6fa41f5c-433c-4185-ae22-7992be8b82ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries extracted and CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# List of US states\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n",
        "    \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
        "    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n",
        "    \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
        "    \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\",\n",
        "    \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/MPC_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "MtzGntW2e5Pw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# D extraction"
      ],
      "metadata": {
        "id": "dGMlHJr8T_fV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_D_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/D_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "0isgDwgMb2xO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/D_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/D_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/D_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_D_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_9uqi1EW3qd",
        "outputId": "6bdcacde-3ae4-49b1-af8b-060beba81fe3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_D_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "        \"Dominion Energy, Inc.\", \"The Company\", \"Company\", \"Company and its subsidiaries\", \"parent\", \"The Companies\", \"Companies\", \"Dominion Energy Virginia\", \"Dominion Energy and Virginia Power\"\n",
        "    ]\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Dominion Energy' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_D_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "5TygHL4qjiPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_D_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries should be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities should be separated by a comma.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_D_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "id": "g3HBwFTfoxlT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_D_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QvPoL5DOo5F4",
        "outputId": "704467df-04af-4afe-abf8-0252ddfe001c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries extracted and CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/D_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "rBpRw0_lbUIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns in a location and uses in the production of goods and services. Examples of physical assets are generating units, power stations and plants.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/D_table.txt'\n",
        "output_txt_path = '/content/D_tables_output.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Srf6qJK-qV_Q",
        "outputId": "084e2652-9367-41b6-817e-48118276f35f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:root:Input text file not found: /content/D_table.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns in a location and uses in the production of goods and services. Examples of physical assets are all examples of 'Plant' in the tables (Wateree, Greensville and Colonial Trail West are all physical assets).\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/D_table_round2.txt'\n",
        "output_txt_path = '/content/D_tables_output_round2.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "hML6RWVDsOui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/D_tables_output.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/D_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/D_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/D_tables_output.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YVFBbt1-8EOu",
        "outputId": "25cd0663-46ba-435a-e1a4-a2094ab66367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DUK extraction"
      ],
      "metadata": {
        "id": "IfeuCQKwys_z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_DUK_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/DUK_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "dTmxi8YHqI7e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/DUK_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/DUK_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/DUK_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_DUK_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbX1rTNKy5CX",
        "outputId": "f51b8abb-e900-49c2-9f15-b02143ec2bed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_DUK_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "         \"Duke Energy subsidiaries\",  \"Duke Energy Corporation\", \"Duke Energy Piedmont\",  \"Duke Energy (partially)\", \"Duke Energy Registrants\", \"Duke Energy RTOs\", \"Duke Energy (wholly or partially)\"\n",
        "    ]\n",
        "    # Split the string by comma and strip whitespace\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Duke Energy' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_DUK_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "bCwIT-5EBIJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_DUK_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries must be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities should be separated by a comma.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_DUK_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-g9ey0mGlxS",
        "outputId": "c1f0c3a3-553d-4a9d-9a1d-dd8a4e30575c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [07:54<00:00, 79.02s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Initialize Wikipedia API\n",
        "wiki_wiki = wikipediaapi.Wikipedia()\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_DUK_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kUwQOtjaGuj2",
        "outputId": "8b9979f7-9b42-4f0f-89b2-6bce9039bff6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries extracted and CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/DUK_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "jbwljTTfbbYH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DUK table parsing"
      ],
      "metadata": {
        "id": "kzlNcR93zTWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns in a location and uses in the production of goods and services. Examples of physical assets are the facility names of electric utilities, infrastructure and electric generation facilities.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/DUK_table.txt'\n",
        "output_txt_path = '/content/DUK_tables_output.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "eyIodo3wzVr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/DUK_tables_output.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/DUK_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)F\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/DUK_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/DUK_tables_output.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PvPMn-tvCcnZ",
        "outputId": "f1834c94-3d71-4c23-858c-bb8dc8a54394"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ED extraction"
      ],
      "metadata": {
        "id": "8bMOAbQFTCI1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_ED_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/ED_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "9xTp0Aq7IOXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/ED_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/ED_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/ED_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_ED_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "id": "EKwoqxFBTupO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_ED_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "         \"Consolidated Edison Company of New York Inc.\", \"Con Edison and its subsidiaries\", \"LLC (CET Electric)\", \"CECONY and other parties\", \"Con Edison Gas Pipeline and Storage\", \"CON EDISON\", \"The Companies\", \"LLC (CET)\", \"Consolidated Edison Inc.\", \"Con Edison\", \"CECONY\", \"Con Edison and CECONY\", \"Con Edison (currently)\", \"Con Edison (partially)\", \"Consolidated Edison, Inc.\"\n",
        "    ]\n",
        "    # Split the string by comma and strip whitespace\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Consolidated Edison' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_ED_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "gIkcOV-0dt-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_ED_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries must be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - All individual organisations in the 'ownership' column should be separated by a comma.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities in the 'commodity' column should be separated by a comma and must be in title case.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_ED_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gSUexY-ypOyk",
        "outputId": "b8d695aa-5d45-4a33-cdc0-d178cbe4d762"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [06:00<00:00, 60.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_ED_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T40Vxh_7vLTV",
        "outputId": "e463f4f7-9833-423c-8500-76f19a0c23b2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries extracted and CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/ED_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "mz9VWaBRb04S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EXC extraction\n"
      ],
      "metadata": {
        "id": "Beleot5gYBbZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_EXC_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/EXC_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "_ViWvsDmwJIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/EXC_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/EXC_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/EXC_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_EXC_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZA73MwlYN3a",
        "outputId": "d61952d9-7968-425e-e72d-a979a6f39660"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_EXC_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "         \"Exelon\", \"Exelon subsidiary\", \"Exelon Generation\", \"xelon Generation Company, LLC (formerly)\", \"Exelon Corporation and Subsidiary Companies\",\n",
        "    ]\n",
        "    # Split the string by comma and strip whitespace\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    cleaned_entities = ['Exelon Corporation' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_EXC_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "UH934N-ijyEH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_EXC_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries must be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - All individual organisations in the 'ownership' column should be separated by a comma. There should not be multiple variations of the same company.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities in the 'commodity' column should be separated by a comma and must be in title case.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_EXC_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nDslVUm8sFwT",
        "outputId": "f7ff8027-de2f-4aaf-ad7d-cc680f565d58"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [06:01<00:00, 60.20s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_EXC_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEngMw3tsujs",
        "outputId": "56fc39bc-36fb-4d75-bfed-ff6e7316f3c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries extracted and CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# List of US states\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n",
        "    \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
        "    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n",
        "    \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
        "    \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\",\n",
        "    \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/EXC_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "goGIUwJrb8W7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import re\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(text):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns in a location and uses in the production of goods and services. Examples of physical assets are all the station names.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {text}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify all the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. Do not leave out any relationships. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def process_document(file_path):\n",
        "    with open(file_path, 'r', encoding='utf-8') as file:\n",
        "        document_text = file.read()\n",
        "\n",
        "    result = query_chunk(document_text)\n",
        "    return result\n",
        "\n",
        "# Specify the path to the input text file\n",
        "input_txt_path = '/content/EXC_table.txt'\n",
        "output_txt_path = '/content/EXC_tables_output_3.txt'\n",
        "\n",
        "# Verify the input file exists\n",
        "if not os.path.exists(input_txt_path):\n",
        "    logging.error(f\"Input text file not found: {input_txt_path}\")\n",
        "else:\n",
        "    logging.info(f\"Input text file found: {input_txt_path}\")\n",
        "\n",
        "    # Process the document\n",
        "    result = process_document(input_txt_path)\n",
        "\n",
        "    # Save the result to a text file\n",
        "    save_results_to_txt(output_txt_path, result)\n",
        "\n",
        "    # Print the generated response\n",
        "    if result:\n",
        "        logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the document:\")\n",
        "        logging.info(result)\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "aqP3J6r491Dl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/EXC_tables_output.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/EXC_tables_output.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/EXC_tables_output.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/EXC_tables_output.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jUlisdEkGS3x",
        "outputId": "4f585d52-59c8-4cec-9046-93a121d8ef1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#NEE extraction"
      ],
      "metadata": {
        "id": "l34THsJy9wDd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sqlite3\n",
        "import logging\n",
        "import time\n",
        "import csv\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Start timing\n",
        "start_time = time.time()\n",
        "\n",
        "os.environ[\"HUGGINGFACE_API_KEY\"] = \"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\"\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "def query_chunk(chunk):\n",
        "    prompt_instruction = (\n",
        "        \"You are a virtual assistant with advanced expertise in a broad spectrum of topics, equipped to utilize high-level critical thinking, cognitive skills, creativity, and innovation.\\n\"\n",
        "        \"Your goal is to deliver the most straightforward and accurate answer possible for each question, ensuring high-quality and useful responses for the user.\\n\"\n",
        "        \"A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. Ensure that a geographical location or region is never considered as an asset.\\n\"\n",
        "        \"A financial asset or other non-physical asset should never be included as a physical asset. Examples of financial assets include equity commitments, corporate facilities, accounts receivable, and short-term investments. Never include these in the list of physical assets.\\n\"\n",
        "        \"A commodity is what the physical asset is being used for. The status of a physical asset gives information on whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"Now, let's analyze the following text:\\n\"\n",
        "        f\"Text: {chunk}\\nQuery: Let's think step-by-step. Does this text mention any physical assets, locations or ownerships? Does the text mention what commodity the physical asset is being used for?\\n\"\n",
        "        \"Does the text mention the status of the physical asset? Examples of status include whether the asset is operational, under construction or in end-of-life.\"\n",
        "        \"If yes, you must specify them in the following format:\\n\"\n",
        "        \"physical assets: [ ]\\nlocations: [ ]\\nownerships: [ ]\\ncommodities: []\\nstatus: []\\n\"\n",
        "        \"Additionally, identify the relationships between them, specifying the location of each physical asset, the ownership details, the commodity the physical asset is used for and the status of the physical asset. \"\n",
        "        \"Format the relationships as follows:\\nrelationships: [asset: '', location: '', ownership: '', commodity: '', status: '']. Do not output anything else.\"\n",
        "    )\n",
        "\n",
        "    prompt = f\"{prompt_instruction}\"\n",
        "    print(100 * '-')\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    output = response\n",
        "\n",
        "    generated_text = output\n",
        "    cleaned_text = cleanup_generated_text(generated_text)\n",
        "    print(cleaned_text)\n",
        "    print(100 * '-')\n",
        "    return cleaned_text.strip()\n",
        "\n",
        "def cleanup_generated_text(text):\n",
        "    \"\"\" Function to remove unwanted conversations and repeated content from generated text. \"\"\"\n",
        "    unwanted_phrases = [\n",
        "        \"assistant\", \"You're welcome\", \"ha\", \"okay\", \"nice\", \"goodbye\", \"thank you\",\n",
        "        \"ahem\", \"we're\", \"I'll\", \"That's\", \"sounds\", \"agreed\", \"chatty\", \"wave\",\n",
        "        \"handshake\", \"hug\", \"laughter\", \"applause\", \"mic drop\", \"explosion\",\n",
        "        \"fireworks\", \"finale\", \"I'm done\", \"let's just stop\", \"ha\", \"same to you\",\n",
        "        \"mission accomplished\", \"excellent\", \"guilty as charged\", \"my secret's safe with you\",\n",
        "        \"under wraps\", \"fun little chat\", \"see you next time\", \"have a great day\",\n",
        "        \"virtual smile\", \"smile\", \"wrapped this up\"\n",
        "    ]\n",
        "    lines = text.split(\"\\n\")\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        if any(phrase.lower() in line.lower() for phrase in unwanted_phrases):\n",
        "            continue\n",
        "        cleaned_lines.append(line)\n",
        "    return \"\\n\".join(cleaned_lines)\n",
        "\n",
        "\n",
        "# Specify the path to the database\n",
        "db_path = '/content/test3_NEE_10-K.db'\n",
        "\n",
        "# Verify the database file exists\n",
        "if not os.path.exists(db_path):\n",
        "    logging.error(f\"Database file not found: {db_path}\")\n",
        "else:\n",
        "    logging.info(f\"Database file found: {db_path}\")\n",
        "\n",
        "    # List all tables in the database to verify the 'filings' table exists\n",
        "    tables = list_tables(db_path)\n",
        "    print(\"Tables in the database:\", tables)\n",
        "\n",
        "    if 'filings' in tables:\n",
        "        # Process the chunks and query for 'assets'\n",
        "        results = process_document_chunks(db_path)\n",
        "\n",
        "\n",
        "        # Save results to a text file\n",
        "        output_txt_path = '/content/NEE_10K_test.txt'\n",
        "        save_results_to_txt(output_txt_path, results)\n",
        "\n",
        "        # Print the generated response\n",
        "        if results:\n",
        "            logging.info(\"Information about 'assets', 'locations', and 'ownerships' found in the following chunks:\")\n",
        "            for chunk_id, result in enumerate(results):\n",
        "                logging.info(f\"Chunk {chunk_id+1}:\\n{result}\")\n",
        "\n",
        "# Log the total time taken\n",
        "end_time = time.time()\n",
        "logging.info(f\"Total time taken: {end_time - start_time} seconds\")\n"
      ],
      "metadata": {
        "id": "BMzr_phOtsYn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "# Define the patterns to extract the relationship information\n",
        "patterns = [\n",
        "    re.compile(r\"asset: '(.*?)', location: '(.*?)', ownership: '(.*?)', commodity: '(.*?)', status: '(.*?)'\"),\n",
        "    re.compile(r\"asset: (.*?), location: (.*?), ownership: (.*?), commodity: (.*?), status: (.*?)(?:\\]|$)\")\n",
        "]\n",
        "\n",
        "# Read the content from the text file\n",
        "with open('/content/NEE_10K_test.txt', 'r') as file:\n",
        "    content = file.read()\n",
        "\n",
        "# Find all matches of the patterns in the content\n",
        "matches = []\n",
        "for pattern in patterns:\n",
        "    matches.extend(pattern.findall(content))\n",
        "\n",
        "# Define the CSV file header\n",
        "header = ['physical asset', 'location', 'ownership', 'commodity', 'status']\n",
        "\n",
        "# Write the matches to a CSV file\n",
        "with open('/content/NEE_10K_test.csv', 'w', newline='') as csvfile:\n",
        "    csvwriter = csv.writer(csvfile)\n",
        "    csvwriter.writerow(header)\n",
        "    for match in matches:\n",
        "        # Clean up the fields and remove unnecessary characters\n",
        "        cleaned_match = [field.strip().strip(\"[]'\") for field in match]\n",
        "        csvwriter.writerow(cleaned_match)\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/NEE_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "# Convert the 'physical asset' column to lowercase\n",
        "data['physical asset'] = data['physical asset'].str.lower()\n",
        "\n",
        "# Identify the duplicated and non-duplicated data\n",
        "duplicated_data = data[data.duplicated('physical asset', keep=False)]\n",
        "non_duplicated_data = data[~data.duplicated('physical asset', keep=False)]\n",
        "\n",
        "# Group by 'physical asset' and combine the other columns for duplicates\n",
        "combined_data = duplicated_data.groupby('physical asset').agg({\n",
        "    'location': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'ownership': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'commodity': lambda x: ', '.join(x.dropna().unique()),\n",
        "    'status': lambda x: ', '.join(x.dropna().unique())\n",
        "}).reset_index()\n",
        "# Concatenate the combined data and the non-duplicated data\n",
        "final_data = pd.concat([combined_data, non_duplicated_data], ignore_index=True)\n",
        "\n",
        "# Save the combined data to a new CSV file\n",
        "final_data.to_csv('/content/combined_NEE_10K_test.csv', index=False)\n",
        "\n",
        "print(\"CSV file combined and saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJb7rXds93BV",
        "outputId": "00b0de69-8e5f-4118-cc4c-0f3ddc36e949"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file combined and saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/combined_NEE_10K_test.csv'\n",
        "data = pd.read_csv(file_path)\n",
        "\n",
        "\n",
        "def replace_company_entities(ownership):\n",
        "    if pd.isna(ownership):\n",
        "        return \"\"\n",
        "    newmont_aliases = [\n",
        "         \"NEER\", \"NEE\", \"NextEra Energy Resources\", \"NEE (NextEra Energy)\", \"NEXTERA ENERGY, INC.\", \"Subsidiaries of NextEra Energy Resources\", \"NEXTERA ENERGY, INC.\", \"Subsidiaries of NEE\", \"NEXTERA ENERGY, INC.\", \"NextEra Energy Resources (controlling interest)\", \"NextEra Energy (NEE)\", \"subsidiaries of NEE\", \"Subsidiaries of NextEra Energy Resources\"\n",
        "    ]\n",
        "    # Split the string by comma and strip whitespace\n",
        "    entities = list(map(str.strip, ownership.split(',')))\n",
        "    # Replace aliases with 'NextEra Energy'\n",
        "    cleaned_entities = ['NextEra Energy' if entity in newmont_aliases else entity for entity in entities]\n",
        "    # Remove duplicates\n",
        "    cleaned_entities = list(dict.fromkeys(cleaned_entities))\n",
        "    return ', '.join(cleaned_entities)\n",
        "\n",
        "data['ownership'] = data['ownership'].apply(replace_company_entities)\n",
        "\n",
        "usa_variants = [\"United States of America\", \"United States\", \"USA\", \"US\", \"USAA\", \"USAA.\", \"U.S.\", \"U.S.A.\"]\n",
        "\n",
        "# Function to normalize USA variants\n",
        "def normalize_usa(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    for variant in usa_variants:\n",
        "        text = text.replace(variant, \"USA\")\n",
        "    return text\n",
        "\n",
        "# Apply the normalize_usa function to the 'location' column\n",
        "data['location'] = data['location'].apply(normalize_usa)\n",
        "\n",
        "# Apply the function to create a new column 'Countries'\n",
        "data['Countries'] = data['location'].apply(extract_country)\n",
        "\n",
        "# Remove rows where 'physical asset' is empty\n",
        "data = data.dropna(subset=['physical asset'])\n",
        "\n",
        "\n",
        "# Function to clean and consolidate entities with case insensitivity\n",
        "def clean_entities(entities):\n",
        "    entities = list(map(str.strip, entities.split(',')))\n",
        "    unique_entities = {}\n",
        "    for entity in entities:\n",
        "        lower_entity = entity.lower()\n",
        "        if lower_entity not in unique_entities or len(entity) > len(unique_entities[lower_entity]):\n",
        "            unique_entities[lower_entity] = entity\n",
        "    return ', '.join(unique_entities.values())\n",
        "\n",
        "# Apply the clean_entities function to relevant columns\n",
        "columns_to_clean = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean:\n",
        "    data[column] = data[column].apply(lambda x: clean_entities(x) if pd.notna(x) else x)\n",
        "\n",
        "\n",
        "# Consolidating similar assets\n",
        "def consolidate_similar_assets(df):\n",
        "    assets = df['physical asset'].tolist()\n",
        "    vectorizer = TfidfVectorizer().fit_transform(assets)\n",
        "    vectors = vectorizer.toarray()\n",
        "    cosine_matrix = cosine_similarity(vectors)\n",
        "\n",
        "    # Threshold for similarity\n",
        "    threshold = 0.5\n",
        "    consolidated_assets = []\n",
        "    used_indices = set()\n",
        "\n",
        "    for i in range(len(assets)):\n",
        "        if i in used_indices:\n",
        "            continue\n",
        "        similar_indices = [j for j in range(len(assets)) if cosine_matrix[i][j] > threshold and i != j]\n",
        "        similar_group = [i] + similar_indices\n",
        "        consolidated_assets.append(similar_group)\n",
        "        used_indices.update(similar_group)\n",
        "\n",
        "    consolidated_results = []\n",
        "\n",
        "    for group in consolidated_assets:\n",
        "        grouped_assets = df.iloc[group]\n",
        "        base_asset = grouped_assets['physical asset'].iloc[0]\n",
        "        consolidated_asset = {\n",
        "            'physical asset': base_asset,\n",
        "            'location': clean_entities(', '.join(grouped_assets['location'].dropna().unique())),\n",
        "            'ownership': clean_entities(', '.join(grouped_assets['ownership'].dropna().unique())),\n",
        "            'commodity': clean_entities(', '.join(grouped_assets['commodity'].dropna().unique())),\n",
        "            'status': clean_entities(', '.join(grouped_assets['status'].dropna().unique())),\n",
        "            'Countries': clean_entities(', '.join(grouped_assets['Countries'].dropna().unique()))\n",
        "        }\n",
        "        consolidated_results.append(consolidated_asset)\n",
        "\n",
        "    return pd.DataFrame(consolidated_results)\n",
        "\n",
        "# Apply the consolidation function\n",
        "data = consolidate_similar_assets(data)\n",
        "\n",
        "\n",
        "# Function to clean and standardize text data\n",
        "def clean_text(text):\n",
        "    if pd.isna(text):\n",
        "        return text\n",
        "    text = text.replace('\\\\', '').replace('\\'', '').replace('\\\"', '').replace(',', ', ').strip()\n",
        "    text = ' '.join(text.split())\n",
        "    return text\n",
        "\n",
        "# Apply the clean_text function to relevant columns\n",
        "columns_to_clean_text = ['location', 'ownership', 'commodity', 'status', 'Countries']\n",
        "for column in columns_to_clean_text:\n",
        "    data[column] = data[column].apply(clean_text)\n",
        "\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/cleaned_NEE_10K_test.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "data.head()\n"
      ],
      "metadata": {
        "id": "VgwpljIcK1ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "\n",
        "# Initialize Ollama model\n",
        "llm = Ollama(model=\"gemma2\")\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/cleaned_NEE_10K_test.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Define the cleaning prompt\n",
        "cleaning_prompt_template = \"\"\"\n",
        "You are an expert data cleaner. Your task is to clean and standardize the following text. You will be provided each cell value one by one with its respective column name. Apply the following cleaning steps:\n",
        "\n",
        "   - Standardize entries in the commodity column to have a consistent format. For example, \"Silver, Gold, Lead, Zinc\" should be the standard format for each commodity, separated by commas and no extra spaces.\n",
        "   - Ensure all entries in the status column are in a consistent format, removing redundant words or phrases.\n",
        "   - All entries must be in title case.\n",
        "   - Do not make any changes to the 'Countries' column.\n",
        "   - In the 'commodity' column, if chemical symbols are given, change these to the element name corresponding to the chemical symbol.\n",
        "   - In all the columns, ensure each entry is properly formatted without redundant commas and extra spaces. For example, \"ExxonMobil\" should not be separated by extra commas.\n",
        "   - All individual organisations in the 'ownership' column should be separated by a comma.\n",
        "   - Remove any leading or trailing spaces in all columns.\n",
        "   - All individual commodities in the 'commodity' column should be separated by a comma and must be in title case.\n",
        "   - The 'location' column should only consist of geographical regions and locations.\n",
        "   - If there are any entries in the ownership column that are not organisations or companies, put N/A in the corresponding cell.\n",
        "   - A physical asset is a tangible resource that a company owns and uses in the production of goods and services. Examples of physical assets are facilities, equipment, infrastructure, etc. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A in the corresponding cell.\n",
        "   - A commodity is what the physical asset is being used for. If there are any entries in the physical asset column that do not fit the description of a physical asset, put N/A next to the word in brackets.\n",
        "   - Ensure that there are no repetitions or redundant entries in any of the cells.\n",
        "   - If any cell has 'not specified', it should be empty.\n",
        "   - All cells should have standardized entries.\n",
        "\n",
        "Process the following text according to these instructions. Return only the new cleaned cell value, nothing else.\n",
        "\"\"\"\n",
        "\n",
        "# Define a function to clean a single cell using the LLM\n",
        "def clean_cell(cell_value, column, model_name='gemma2'):\n",
        "    prompt = cleaning_prompt_template + f\"\\ncolumn name: {column}\" +  f\"\\n cell value: {cell_value}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_cell = response.strip()\n",
        "    return cleaned_cell\n",
        "\n",
        "# Iterate through each cell in the DataFrame with nested tqdm progress bars\n",
        "for column in tqdm(df.columns, desc=\"Columns\"):\n",
        "    for index, value in df[column].items():\n",
        "        if pd.notna(value) and value != '':\n",
        "            df.at[index, column] = clean_cell(value, column)\n",
        "\n",
        "\n",
        "# Save the cleaned dataframe to a new CSV file\n",
        "output_file_path = '/content/LLMcleaned_NEE_10K_test.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"CSV file cleaned and saved successfully.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3CBbyBb_Bud",
        "outputId": "def610c0-b1e2-43ab-9f30-36deb55513f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Columns: 100%|██████████| 6/6 [05:12<00:00, 52.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file cleaned and saved successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from langchain_community.llms import Ollama\n",
        "from llama_index.core.node_parser import SentenceSplitter\n",
        "from llama_index.core import Document\n",
        "import wikipediaapi\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/LLMcleaned_NEE_10K_test.csv'\n",
        "df = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# Define the country extraction prompt\n",
        "country_extraction_prompt = \"\"\"\n",
        "You are an expert in geographical locations. Given the location information provided, identify the countries mentioned in the location. Return the list of countries separated by commas. If no country is mentioned, return \"N/A\".\"\"\"\n",
        "\n",
        "\n",
        "# Define a function to extract countries using the LLM\n",
        "def extract_countries(location):\n",
        "    if pd.isna(location) or location.strip() == \"\":\n",
        "        return \"N/A\"\n",
        "\n",
        "    prompt = country_extraction_prompt + f\"\\nLocation: {location}\"\n",
        "    response = llm.invoke(input=[{'role': 'user', 'content': prompt}])\n",
        "    cleaned_response = response.strip()\n",
        "    return cleaned_response\n",
        "\n",
        "# Define a function to verify the country using Wikipedia\n",
        "def verify_country_with_wikipedia(location, extracted_country):\n",
        "    page = wiki_wiki.page(location)\n",
        "    if not page.exists():\n",
        "        return extracted_country\n",
        "    summary = page.summary.lower()\n",
        "    for country in extracted_country.split(','):\n",
        "        if country.strip().lower() in summary:\n",
        "            return extracted_country\n",
        "    return \"N/A\"\n",
        "\n",
        "# Apply the function to the 'location' column and store results in the 'Countries' column\n",
        "def process_location(location):\n",
        "    extracted_countries = extract_countries(location)\n",
        "    verified_countries = verify_country_with_wikipedia(location, extracted_countries)\n",
        "    return verified_countries\n",
        "\n",
        "df['Countries'] = df['location'].apply(process_location)\n",
        "\n",
        "# Save the updated DataFrame to a new CSV file\n",
        "output_file_path = '/content/countries_extracted.csv'\n",
        "df.to_csv(output_file_path, index=False)\n",
        "\n",
        "print(\"Countries extracted and CSV file saved successfully.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_WnYrt1qQFJk",
        "outputId": "55c2dd3e-da50-4904-a1e1-d65537ffdba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Countries extracted and CSV file saved successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the CSV file\n",
        "file_path = '/content/FINAL.csv'\n",
        "data = pd.read_csv(file_path, encoding='latin1')\n",
        "\n",
        "# List of US states\n",
        "us_states = [\n",
        "    \"Alabama\", \"Alaska\", \"Arizona\", \"Arkansas\", \"California\", \"Colorado\", \"Connecticut\",\n",
        "    \"Delaware\", \"Florida\", \"Georgia\", \"Hawaii\", \"Idaho\", \"Illinois\", \"Indiana\", \"Iowa\",\n",
        "    \"Kansas\", \"Kentucky\", \"Louisiana\", \"Maine\", \"Maryland\", \"Massachusetts\", \"Michigan\",\n",
        "    \"Minnesota\", \"Mississippi\", \"Missouri\", \"Montana\", \"Nebraska\", \"Nevada\", \"New Hampshire\",\n",
        "    \"New Jersey\", \"New Mexico\", \"New York\", \"North Carolina\", \"North Dakota\", \"Ohio\", \"Oklahoma\",\n",
        "    \"Oregon\", \"Pennsylvania\", \"Rhode Island\", \"South Carolina\", \"South Dakota\", \"Tennessee\", \"Texas\",\n",
        "    \"Utah\", \"Vermont\", \"Virginia\", \"Washington\", \"West Virginia\", \"Wisconsin\", \"Wyoming\"\n",
        "]\n",
        "\n",
        "# Function to extract US states from the location column\n",
        "def extract_us_states(location):\n",
        "    if pd.isna(location):\n",
        "        return None\n",
        "    found_states = [state for state in us_states if state in location]\n",
        "    return ', '.join(found_states) if found_states else None\n",
        "\n",
        "# Apply the function to the location column\n",
        "data['USA states'] = data['location'].apply(extract_us_states)\n",
        "\n",
        "# Save the updated dataframe to a new CSV file\n",
        "output_file_path = '/content/NEE_w_states.csv'\n",
        "data.to_csv(output_file_path, index=False)\n",
        "\n",
        "# Display the updated dataframe\n",
        "print(data.head())\n"
      ],
      "metadata": {
        "id": "FrVNkvq-Q9qn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}