{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53a8b392-6d74-4262-9bbf-d524c0ee5726",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nest_asyncio -U -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6911a6e5-bd68-4fce-9335-1033fbfd5a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/sec-edgar/sec-edgar?tab=readme-ov-file\n",
    "# pull from the above repo first before running this code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "667607eb-64d6-46da-a1cf-18d6358c526d",
   "metadata": {},
   "source": [
    "# downloading filings for oil and gas companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aacf5c53-d5d6-4d24-b699-e44b937dabe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 12:09:41,657 - INFO - Attempting to download FILING_10Q filings for ticker XOM (Attempt 1)\n",
      "C:\\Users\\avani\\Desktop\\Thesis\\sec-edgar\\secedgar\\client.py:218: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  return BeautifulSoup(self.get_response(path, params, **kwargs).text,\n",
      "310it [00:31,  9.92it/s]                                                                                               \n",
      "2024-07-02 12:10:15,487 - INFO - Successfully downloaded FILING_10Q filings for ticker XOM\n",
      "2024-07-02 12:10:16,491 - INFO - Attempting to download FILING_10K filings for ticker XOM (Attempt 1)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:06<00:00,  8.98it/s]\n",
      "2024-07-02 12:10:24,267 - INFO - Successfully downloaded FILING_10K filings for ticker XOM\n",
      "2024-07-02 12:10:25,268 - INFO - Attempting to download FILING_8K filings for ticker XOM (Attempt 1)\n",
      "1140it [01:55,  9.90it/s]                                                                                              \n",
      "2024-07-02 12:12:27,446 - INFO - Successfully downloaded FILING_8K filings for ticker XOM\n",
      "2024-07-02 12:12:27,454 - INFO - Attempting to download FILING_10Q filings for ticker CVX (Attempt 1)\n",
      "310it [00:31,  9.91it/s]                                                                                               \n",
      "2024-07-02 12:13:01,080 - INFO - Successfully downloaded FILING_10Q filings for ticker CVX\n",
      "2024-07-02 12:13:02,088 - INFO - Attempting to download FILING_10K filings for ticker CVX (Attempt 1)\n",
      "60it [00:07,  8.35it/s]                                                                                                \n",
      "2024-07-02 12:13:10,630 - INFO - Successfully downloaded FILING_10K filings for ticker CVX\n",
      "2024-07-02 12:13:11,641 - INFO - Attempting to download FILING_8K filings for ticker CVX (Attempt 1)\n",
      "1140it [01:54,  9.92it/s]                                                                                              \n",
      "2024-07-02 12:15:14,765 - INFO - Successfully downloaded FILING_8K filings for ticker CVX\n",
      "2024-07-02 12:15:14,766 - INFO - Attempting to download FILING_10Q filings for ticker COP (Attempt 1)\n",
      "210it [00:22,  9.49it/s]                                                                                               \n",
      "2024-07-02 12:15:38,519 - INFO - Successfully downloaded FILING_10Q filings for ticker COP\n",
      "2024-07-02 12:15:39,528 - INFO - Attempting to download FILING_10K filings for ticker COP (Attempt 1)\n",
      "40it [00:04,  8.47it/s]                                                                                                \n",
      "2024-07-02 12:15:45,283 - INFO - Successfully downloaded FILING_10K filings for ticker COP\n",
      "2024-07-02 12:15:46,285 - INFO - Attempting to download FILING_8K filings for ticker COP (Attempt 1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 920/920 [01:32<00:00,  9.91it/s]\n",
      "2024-07-02 12:17:25,681 - INFO - Successfully downloaded FILING_8K filings for ticker COP\n",
      "2024-07-02 12:17:25,681 - INFO - Attempting to download FILING_10Q filings for ticker OXY (Attempt 1)\n",
      "300it [00:30,  9.98it/s]                                                                                               \n",
      "2024-07-02 12:17:57,902 - INFO - Successfully downloaded FILING_10Q filings for ticker OXY\n",
      "2024-07-02 12:17:58,905 - INFO - Attempting to download FILING_10K filings for ticker OXY (Attempt 1)\n",
      "50it [00:05,  8.77it/s]                                                                                                \n",
      "2024-07-02 12:18:05,793 - INFO - Successfully downloaded FILING_10K filings for ticker OXY\n",
      "2024-07-02 12:18:06,796 - INFO - Attempting to download FILING_8K filings for ticker OXY (Attempt 1)\n",
      "1470it [02:27,  9.96it/s]                                                                                              \n",
      "2024-07-02 12:20:46,011 - INFO - Successfully downloaded FILING_8K filings for ticker OXY\n",
      "2024-07-02 12:20:46,012 - INFO - Attempting to download FILING_10Q filings for ticker SLB (Attempt 1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [00:28<00:00,  9.98it/s]\n",
      "2024-07-02 12:21:16,739 - INFO - Successfully downloaded FILING_10Q filings for ticker SLB\n",
      "2024-07-02 12:21:17,747 - INFO - Attempting to download FILING_10K filings for ticker SLB (Attempt 1)\n",
      "40it [00:04,  9.39it/s]                                                                                                \n",
      "2024-07-02 12:21:22,918 - INFO - Successfully downloaded FILING_10K filings for ticker SLB\n",
      "2024-07-02 12:21:23,921 - INFO - Attempting to download FILING_8K filings for ticker SLB (Attempt 1)\n",
      "1070it [01:47,  9.91it/s]                                                                                              \n",
      "2024-07-02 12:23:18,660 - INFO - Successfully downloaded FILING_8K filings for ticker SLB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filings download process completed.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import logging\n",
    "from secedgar import filings, FilingType\n",
    "from secedgar.exceptions import EDGARQueryError, NoFilingsError\n",
    "from time import sleep\n",
    "\n",
    "# Apply the nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"filings_download.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define the tickers for the specified utility companies\n",
    "utility_tickers = [\n",
    "    \"XOM\",  # Exxon Mobil Corporation\n",
    "    \"CVX\",  # Chevron Corporation\n",
    "    \"COP\",   # ConocoPhillips\n",
    "    \"OXY\",  # Occidental Petroleum Corporation\n",
    "    \"MPC\"    # Marathon Petroleum\n",
    "]\n",
    "# Define the maximum number of retries\n",
    "max_retries = 3\n",
    "\n",
    "# Function to download filings with retry logic and logging\n",
    "def download_filings(ticker, filing_type, base_path, user_agent):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to download {filing_type.name} filings for ticker {ticker} (Attempt {attempt + 1})\")\n",
    "            my_filings = filings(cik_lookup=[ticker],\n",
    "                                 filing_type=filing_type,\n",
    "                                 user_agent=user_agent)\n",
    "            my_filings.save(base_path)\n",
    "            logging.info(f\"Successfully downloaded {filing_type.name} filings for ticker {ticker}\")\n",
    "            break\n",
    "        except (EDGARQueryError, NoFilingsError) as e:\n",
    "            logging.error(f\"Error downloading {filing_type.name} filings for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "\n",
    "# Define the base path where the filings are saved\n",
    "base_path = r'C:\\Users\\avani\\Desktop\\Thesis\\oilandgas'\n",
    "user_agent = \" \"\n",
    "\n",
    "# Download filings for each company \n",
    "for ticker in utility_tickers:\n",
    "    download_filings(ticker, FilingType.FILING_10Q, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_10K, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_8K, base_path, user_agent)\n",
    "\n",
    "print(\"Filings download process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e24de58-e09c-40b6-8ede-6ddb3834558c",
   "metadata": {},
   "source": [
    "# downloading filings for utilities companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b40a4060-4fe7-4e33-820c-66cf6b43347b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 12:23:18,692 - INFO - Attempting to download FILING_10Q filings for ticker NEE (Attempt 1)\n",
      "310it [00:36,  8.60it/s]                                                                                               \n",
      "2024-07-02 12:23:56,950 - INFO - Successfully downloaded FILING_10Q filings for ticker NEE\n",
      "2024-07-02 12:23:57,953 - INFO - Attempting to download FILING_10K filings for ticker NEE (Attempt 1)\n",
      "70it [00:09,  7.72it/s]                                                                                                \n",
      "2024-07-02 12:24:08,216 - INFO - Successfully downloaded FILING_10K filings for ticker NEE\n",
      "2024-07-02 12:24:09,218 - INFO - Attempting to download FILING_8K filings for ticker NEE (Attempt 1)\n",
      "1830it [03:04,  9.92it/s]                                                                                              \n",
      "2024-07-02 12:27:25,305 - INFO - Successfully downloaded FILING_8K filings for ticker NEE\n",
      "2024-07-02 12:27:25,305 - INFO - Attempting to download FILING_10Q filings for ticker DUK (Attempt 1)\n",
      "170it [00:44,  3.84it/s]                                                                                               \n",
      "2024-07-02 12:28:11,097 - INFO - Successfully downloaded FILING_10Q filings for ticker DUK\n",
      "2024-07-02 12:28:12,106 - INFO - Attempting to download FILING_10K filings for ticker DUK (Attempt 1)\n",
      "30it [00:09,  3.01it/s]                                                                                                \n",
      "2024-07-02 12:28:22,752 - INFO - Successfully downloaded FILING_10K filings for ticker DUK\n",
      "2024-07-02 12:28:23,759 - INFO - Attempting to download FILING_8K filings for ticker DUK (Attempt 1)\n",
      "1740it [02:55,  9.94it/s]                                                                                              \n",
      "2024-07-02 12:31:30,467 - INFO - Successfully downloaded FILING_8K filings for ticker DUK\n",
      "2024-07-02 12:31:30,472 - INFO - Attempting to download FILING_10Q filings for ticker EXC (Attempt 1)\n",
      "230it [01:26,  2.66it/s]                                                                                               \n",
      "2024-07-02 12:32:59,385 - INFO - Successfully downloaded FILING_10Q filings for ticker EXC\n",
      "2024-07-02 12:33:00,396 - INFO - Attempting to download FILING_10K filings for ticker EXC (Attempt 1)\n",
      "50it [00:25,  1.98it/s]                                                                                                \n",
      "2024-07-02 12:33:26,615 - INFO - Successfully downloaded FILING_10K filings for ticker EXC\n",
      "2024-07-02 12:33:27,635 - INFO - Attempting to download FILING_8K filings for ticker EXC (Attempt 1)\n",
      "2910it [04:58,  9.75it/s]                                                                                              \n",
      "2024-07-02 12:38:46,516 - INFO - Successfully downloaded FILING_8K filings for ticker EXC\n",
      "2024-07-02 12:38:46,516 - INFO - Attempting to download FILING_10Q filings for ticker PCG (Attempt 1)\n",
      "270it [00:27,  9.87it/s]                                                                                               \n",
      "2024-07-02 12:39:16,114 - INFO - Successfully downloaded FILING_10Q filings for ticker PCG\n",
      "2024-07-02 12:39:17,120 - INFO - Attempting to download FILING_10K filings for ticker PCG (Attempt 1)\n",
      "60it [00:07,  7.91it/s]                                                                                                \n",
      "2024-07-02 12:39:26,091 - INFO - Successfully downloaded FILING_10K filings for ticker PCG\n",
      "2024-07-02 12:39:27,101 - INFO - Attempting to download FILING_8K filings for ticker PCG (Attempt 1)\n",
      "3300it [05:47,  9.51it/s]                                                                                              \n",
      "2024-07-02 12:45:39,149 - INFO - Successfully downloaded FILING_8K filings for ticker PCG\n",
      "2024-07-02 12:45:39,149 - INFO - Attempting to download FILING_10Q filings for ticker ED (Attempt 1)\n",
      "260it [00:30,  8.62it/s]                                                                                               \n",
      "2024-07-02 12:46:11,649 - INFO - Successfully downloaded FILING_10Q filings for ticker ED\n",
      "2024-07-02 12:46:12,657 - INFO - Attempting to download FILING_10K filings for ticker ED (Attempt 1)\n",
      "50it [00:07,  6.50it/s]                                                                                                \n",
      "2024-07-02 12:46:21,501 - INFO - Successfully downloaded FILING_10K filings for ticker ED\n",
      "2024-07-02 12:46:22,506 - INFO - Attempting to download FILING_8K filings for ticker ED (Attempt 1)\n",
      "980it [01:41,  9.70it/s]                                                                                               \n",
      "2024-07-02 12:48:10,949 - INFO - Successfully downloaded FILING_8K filings for ticker ED\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filings download process completed.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import logging\n",
    "from secedgar import filings, FilingType\n",
    "from secedgar.exceptions import EDGARQueryError, NoFilingsError\n",
    "from time import sleep\n",
    "\n",
    "# Apply the nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"filings_download.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define the tickers for the specified utility companies\n",
    "utility_tickers = [\n",
    "    \"NEE\",  # NextEra Energy, Inc.\n",
    "    \"DUK\",  # Duke Energy Corporation\n",
    "    \"EXC\",   # Exelon corporation\n",
    "    \"D\",  # Dominion\n",
    "    \"ED\"   # Consolidated Edison, Inc.\n",
    "]\n",
    "# Define the maximum number of retries\n",
    "max_retries = 3\n",
    "\n",
    "# Function to download filings with retry logic and logging\n",
    "def download_filings(ticker, filing_type, base_path, user_agent):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to download {filing_type.name} filings for ticker {ticker} (Attempt {attempt + 1})\")\n",
    "            my_filings = filings(cik_lookup=[ticker],\n",
    "                                 filing_type=filing_type,\n",
    "                                 user_agent=user_agent)\n",
    "            my_filings.save(base_path)\n",
    "            logging.info(f\"Successfully downloaded {filing_type.name} filings for ticker {ticker}\")\n",
    "            break\n",
    "        except (EDGARQueryError, NoFilingsError) as e:\n",
    "            logging.error(f\"Error downloading {filing_type.name} filings for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "\n",
    "# Define the base path where the filings are saved\n",
    "base_path = r'C:\\Users\\avani\\Desktop\\Thesis\\utilities'\n",
    "user_agent = \"\"\n",
    "\n",
    "# Download filings for each company\n",
    "for ticker in utility_tickers:\n",
    "    download_filings(ticker, FilingType.FILING_10Q, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_10K, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_8K, base_path, user_agent)\n",
    "\n",
    "print(\"Filings download process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff02c7a4-e3e4-413f-a3dd-6a89bd4d90ff",
   "metadata": {},
   "source": [
    "## downloading filings for mining companies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e31a7126-3a2b-4369-a5dd-49cfc199281c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-19 22:51:30,307 - INFO - Attempting to download FILING_10Q filings for ticker FCX (Attempt 1)\n",
      "C:\\Users\\avani\\anaconda3\\Lib\\site-packages\\bs4\\builder\\__init__.py:545: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  warnings.warn(\n",
      "310it [00:45,  6.85it/s]                                                                                               \n",
      "2024-06-19 22:52:26,996 - INFO - Successfully downloaded FILING_10Q filings for ticker FCX\n",
      "2024-06-19 22:52:27,998 - INFO - Attempting to download FILING_10K filings for ticker FCX (Attempt 1)\n",
      "50it [00:17,  2.92it/s]                                                                                                \n",
      "2024-06-19 22:52:49,294 - INFO - Successfully downloaded FILING_10K filings for ticker FCX\n",
      "2024-06-19 22:52:50,299 - INFO - Attempting to download FILING_8K filings for ticker FCX (Attempt 1)\n",
      "1610it [02:50,  9.42it/s]                                                                                              \n",
      "2024-06-19 22:56:33,678 - INFO - Successfully downloaded FILING_8K filings for ticker FCX\n",
      "2024-06-19 22:56:33,681 - INFO - Attempting to download FILING_10Q filings for ticker NEM (Attempt 1)\n",
      "210it [00:39,  5.34it/s]                                                                                               \n",
      "2024-06-19 22:57:21,235 - INFO - Successfully downloaded FILING_10Q filings for ticker NEM\n",
      "2024-06-19 22:57:22,239 - INFO - Attempting to download FILING_10K filings for ticker NEM (Attempt 1)\n",
      "50it [00:19,  2.63it/s]                                                                                                \n",
      "2024-06-19 22:57:45,760 - INFO - Successfully downloaded FILING_10K filings for ticker NEM\n",
      "2024-06-19 22:57:46,762 - INFO - Attempting to download FILING_8K filings for ticker NEM (Attempt 1)\n",
      "1440it [02:32,  9.42it/s]                                                                                              \n",
      "2024-06-19 23:01:05,783 - INFO - Successfully downloaded FILING_8K filings for ticker NEM\n",
      "2024-06-19 23:01:05,792 - INFO - Attempting to download FILING_10Q filings for ticker SCCO (Attempt 1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [00:30<00:00,  9.04it/s]\n",
      "2024-06-19 23:01:48,087 - INFO - Successfully downloaded FILING_10Q filings for ticker SCCO\n",
      "2024-06-19 23:01:49,093 - INFO - Attempting to download FILING_10K filings for ticker SCCO (Attempt 1)\n",
      "60it [00:09,  6.01it/s]                                                                                                \n",
      "2024-06-19 23:02:05,641 - INFO - Successfully downloaded FILING_10K filings for ticker SCCO\n",
      "2024-06-19 23:02:06,644 - INFO - Attempting to download FILING_8K filings for ticker SCCO (Attempt 1)\n",
      "970it [01:37,  9.92it/s]                                                                                               \n",
      "2024-06-19 23:04:12,483 - INFO - Successfully downloaded FILING_8K filings for ticker SCCO\n",
      "2024-06-19 23:04:12,485 - INFO - Attempting to download FILING_10Q filings for ticker AA (Attempt 1)\n",
      "40it [00:04,  8.33it/s]                                                                                                \n",
      "2024-06-19 23:04:21,632 - INFO - Successfully downloaded FILING_10Q filings for ticker AA\n",
      "2024-06-19 23:04:22,635 - INFO - Attempting to download FILING_10K filings for ticker AA (Attempt 1)\n",
      "10it [00:08,  1.24it/s]\n",
      "2024-06-19 23:04:32,892 - INFO - Successfully downloaded FILING_10K filings for ticker AA\n",
      "2024-06-19 23:04:33,895 - INFO - Attempting to download FILING_8K filings for ticker AA (Attempt 1)\n",
      "430it [00:43,  9.88it/s]                                                                                               \n",
      "2024-06-19 23:05:35,463 - INFO - Successfully downloaded FILING_8K filings for ticker AA\n",
      "2024-06-19 23:05:35,466 - INFO - Attempting to download FILING_10Q filings for ticker HL (Attempt 1)\n",
      "310it [00:31,  9.83it/s]                                                                                               \n",
      "2024-06-19 23:06:16,620 - INFO - Successfully downloaded FILING_10Q filings for ticker HL\n",
      "2024-06-19 23:06:17,627 - INFO - Attempting to download FILING_10K filings for ticker HL (Attempt 1)\n",
      "50it [00:09,  5.20it/s]                                                                                                \n",
      "2024-06-19 23:06:32,258 - INFO - Successfully downloaded FILING_10K filings for ticker HL\n",
      "2024-06-19 23:06:33,264 - INFO - Attempting to download FILING_8K filings for ticker HL (Attempt 1)\n",
      "1450it [02:28,  9.79it/s]                                                                                              \n",
      "2024-06-19 23:09:38,586 - INFO - Successfully downloaded FILING_8K filings for ticker HL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filings download process completed.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import logging\n",
    "from secedgar import filings, FilingType\n",
    "from secedgar.exceptions import EDGARQueryError, NoFilingsError\n",
    "from time import sleep\n",
    "\n",
    "# Apply the nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"filings_download.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define the tickers for the specified utility companie\n",
    "utility_tickers = [\n",
    "    \"FCX\",  # Freeport-McMoRan Inc\n",
    "    \"NEM\",  # Newmont Corporation\n",
    "    \"SCCO\",  # Southern Copper Corporation\n",
    "    \"AA\",    # Alcoa Corporation\n",
    "    \"HL\"   # Hecla Mining Company\n",
    "]\n",
    "# Define the maximum number of retries\n",
    "max_retries = 3\n",
    "\n",
    "# Function to download filings with retry logic and logging\n",
    "def download_filings(ticker, filing_type, base_path, user_agent):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to download {filing_type.name} filings for ticker {ticker} (Attempt {attempt + 1})\")\n",
    "            my_filings = filings(cik_lookup=[ticker],\n",
    "                                 filing_type=filing_type,\n",
    "                                 user_agent=user_agent)\n",
    "            my_filings.save(base_path)\n",
    "            logging.info(f\"Successfully downloaded {filing_type.name} filings for ticker {ticker}\")\n",
    "            break\n",
    "        except (EDGARQueryError, NoFilingsError) as e:\n",
    "            logging.error(f\"Error downloading {filing_type.name} filings for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "\n",
    "# Define the base path where the filings are saved\n",
    "base_path = r'C:\\Users\\avani\\Desktop\\Thesis\\mining'\n",
    "user_agent = \"\"\n",
    "\n",
    "# Download filings for each company \n",
    "for ticker in utility_tickers:\n",
    "    download_filings(ticker, FilingType.FILING_10Q, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_10K, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_8K, base_path, user_agent)\n",
    "\n",
    "print(\"Filings download process completed.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22daa2c0-0f6b-4cc8-8696-cecdf8e5eefa",
   "metadata": {},
   "source": [
    "# Cleaning without removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17e6e34f-0803-4cad-8349-7872b9b68a79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-27 16:47:06,471 - INFO - Attempting to download FILING_10Q filings for ticker FCX (Attempt 1)\n",
      "C:\\Users\\avani\\Desktop\\Thesis\\sec-edgar\\secedgar\\client.py:218: XMLParsedAsHTMLWarning: It looks like you're parsing an XML document using an HTML parser. If this really is an HTML document (maybe it's XHTML?), you can ignore or filter this warning. If it's XML, you should know that using an XML parser will be more reliable. To parse this document as XML, make sure you have the lxml package installed, and pass the keyword argument `features=\"xml\"` into the BeautifulSoup constructor.\n",
      "  return BeautifulSoup(self.get_response(path, params, **kwargs).text,\n",
      "310it [00:48,  6.46it/s]                                                                                               \n",
      "2024-07-27 16:47:59,129 - INFO - Successfully downloaded FILING_10Q filings for ticker FCX\n",
      "2024-07-27 16:48:00,136 - INFO - Attempting to download FILING_10K filings for ticker FCX (Attempt 1)\n",
      "50it [00:17,  2.82it/s]                                                                                                \n",
      "2024-07-27 16:48:19,373 - INFO - Successfully downloaded FILING_10K filings for ticker FCX\n",
      "2024-07-27 16:48:20,376 - INFO - Attempting to download FILING_8K filings for ticker FCX (Attempt 1)\n",
      "1620it [02:47,  9.69it/s]                                                                                              \n",
      "2024-07-27 16:51:19,093 - INFO - Successfully downloaded FILING_8K filings for ticker FCX\n",
      "2024-07-27 16:51:19,098 - INFO - Attempting to download FILING_10Q filings for ticker NEM (Attempt 1)\n",
      "220it [00:45,  4.81it/s]                                                                                               \n",
      "2024-07-27 16:52:07,471 - INFO - Successfully downloaded FILING_10Q filings for ticker NEM\n",
      "2024-07-27 16:52:08,473 - INFO - Attempting to download FILING_10K filings for ticker NEM (Attempt 1)\n",
      "50it [00:22,  2.27it/s]                                                                                                \n",
      "2024-07-27 16:52:31,620 - INFO - Successfully downloaded FILING_10K filings for ticker NEM\n",
      "2024-07-27 16:52:32,644 - INFO - Attempting to download FILING_8K filings for ticker NEM (Attempt 1)\n",
      "1450it [02:30,  9.64it/s]                                                                                              \n",
      "2024-07-27 16:55:15,860 - INFO - Successfully downloaded FILING_8K filings for ticker NEM\n",
      "2024-07-27 16:55:15,860 - INFO - Attempting to download FILING_10Q filings for ticker SCCO (Attempt 1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [00:31<00:00,  8.92it/s]\n",
      "2024-07-27 16:55:50,392 - INFO - Successfully downloaded FILING_10Q filings for ticker SCCO\n",
      "2024-07-27 16:55:51,400 - INFO - Attempting to download FILING_10K filings for ticker SCCO (Attempt 1)\n",
      "60it [00:12,  4.62it/s]                                                                                                \n",
      "2024-07-27 16:56:06,277 - INFO - Successfully downloaded FILING_10K filings for ticker SCCO\n",
      "2024-07-27 16:56:07,290 - INFO - Attempting to download FILING_8K filings for ticker SCCO (Attempt 1)\n",
      "980it [01:39,  9.89it/s]                                                                                               \n",
      "2024-07-27 16:57:54,528 - INFO - Successfully downloaded FILING_8K filings for ticker SCCO\n",
      "2024-07-27 16:57:54,528 - INFO - Attempting to download FILING_10Q filings for ticker AA (Attempt 1)\n",
      "40it [00:05,  7.82it/s]                                                                                                \n",
      "2024-07-27 16:58:00,649 - INFO - Successfully downloaded FILING_10Q filings for ticker AA\n",
      "2024-07-27 16:58:01,652 - INFO - Attempting to download FILING_10K filings for ticker AA (Attempt 1)\n",
      "10it [00:06,  1.48it/s]\n",
      "2024-07-27 16:58:08,787 - INFO - Successfully downloaded FILING_10K filings for ticker AA\n",
      "2024-07-27 16:58:09,792 - INFO - Attempting to download FILING_8K filings for ticker AA (Attempt 1)\n",
      "450it [00:46,  9.77it/s]                                                                                               \n",
      "2024-07-27 16:58:59,463 - INFO - Successfully downloaded FILING_8K filings for ticker AA\n",
      "2024-07-27 16:58:59,463 - INFO - Attempting to download FILING_10Q filings for ticker HL (Attempt 1)\n",
      "310it [00:33,  9.35it/s]                                                                                               \n",
      "2024-07-27 16:59:36,205 - INFO - Successfully downloaded FILING_10Q filings for ticker HL\n",
      "2024-07-27 16:59:37,220 - INFO - Attempting to download FILING_10K filings for ticker HL (Attempt 1)\n",
      "50it [00:11,  4.43it/s]                                                                                                \n",
      "2024-07-27 16:59:50,085 - INFO - Successfully downloaded FILING_10K filings for ticker HL\n",
      "2024-07-27 16:59:51,090 - INFO - Attempting to download FILING_8K filings for ticker HL (Attempt 1)\n",
      "1450it [02:28,  9.78it/s]                                                                                              \n",
      "2024-07-27 17:02:32,691 - INFO - Successfully downloaded FILING_8K filings for ticker HL\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filings download process completed.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 8-K filings of fcx.\n",
      "Data has been written to FCX_8-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-K filings of fcx.\n",
      "Data has been written to FCX_10-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-Q filings of fcx.\n",
      "Data has been written to FCX_10-Q.txt.\n",
      "All filings processed.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import logging\n",
    "from secedgar import filings, FilingType\n",
    "from secedgar.exceptions import EDGARQueryError, NoFilingsError\n",
    "from time import sleep\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Apply the nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"filings_download.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define the tickers for the specified utility companies\n",
    "utility_tickers = [\n",
    "    \"FCX\",  # Freeport-McMoRan Inc\n",
    "    \"NEM\",  # Newmont Corporation\n",
    "    \"SCCO\",  # Southern Copper Corporation\n",
    "    \"AA\",    # Alcoa Corporation\n",
    "    \"HL\"     # Hecla Mining Company\n",
    "]\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 3\n",
    "\n",
    "# Function to download filings with retry logic and logging\n",
    "def download_filings(ticker, filing_type, base_path, user_agent):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to download {filing_type.name} filings for ticker {ticker} (Attempt {attempt + 1})\")\n",
    "            my_filings = filings(cik_lookup=[ticker],\n",
    "                                 filing_type=filing_type,\n",
    "                                 user_agent=user_agent)\n",
    "            my_filings.save(base_path)\n",
    "            logging.info(f\"Successfully downloaded {filing_type.name} filings for ticker {ticker}\")\n",
    "            break\n",
    "        except (EDGARQueryError, NoFilingsError) as e:\n",
    "            logging.error(f\"Error downloading {filing_type.name} filings for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "\n",
    "# Download filings for each company \n",
    "for ticker in utility_tickers:\n",
    "    download_filings(ticker, FilingType.FILING_10Q, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_10K, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_8K, base_path, user_agent)\n",
    "\n",
    "print(\"Filings download process completed.\")\n",
    "\n",
    "def extract_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    company_name = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)', content)\n",
    "    company_name = company_name.group(1) if company_name else None\n",
    "\n",
    "    filing_date = re.search(r'<ACCEPTANCE-DATETIME>(\\d+)', content)\n",
    "    filing_date = filing_date.group(1) if filing_date else None\n",
    "\n",
    "    accession_number = re.search(r'ACCESSION NUMBER:\\s*(.+)', content)\n",
    "    accession_number = accession_number.group(1) if accession_number else None\n",
    "\n",
    "    form_type = re.search(r'FORM TYPE:\\s*(.+)', content)\n",
    "    form_type = form_type.group(1) if form_type else None\n",
    "\n",
    "    business_address = re.search(r'BUSINESS ADDRESS:\\s+STREET 1:\\s+(.+)\\s+STREET 2:\\s+(.+)\\s+CITY:\\s+(.+)\\s+STATE:\\s+(.+)\\s+ZIP:\\s+(\\d+)', content)\n",
    "    business_address = f\"{business_address.group(1)}, {business_address.group(2)}, {business_address.group(3)}, {business_address.group(4)}, {business_address.group(5)}\" if business_address else None\n",
    "\n",
    "    document_type = re.search(r'<TYPE>(.+)', content)\n",
    "    document_type = document_type.group(1) if document_type else None\n",
    "\n",
    "    document_text = re.search(r'<TEXT>(.*?)</TEXT>', content, re.DOTALL)\n",
    "    document_text = document_text.group(1).strip() if document_text else None\n",
    "\n",
    "    if document_text:\n",
    "        document_text = clean_html(document_text)\n",
    "\n",
    "    return {\n",
    "        'company_name': company_name,\n",
    "        'filing_date': filing_date,\n",
    "        'accession_number': accession_number,\n",
    "        'form_type': form_type,\n",
    "        'business_address': business_address,\n",
    "        'document_type': document_type,\n",
    "        'document_text': document_text,\n",
    "    }\n",
    "\n",
    "def clean_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    \n",
    "  \n",
    "    # Remove unwanted characters and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_document_text(document_text):\n",
    "    financial_statement_pattern = re.compile(r'(?<=financial statements:)(.*?)(?=end of financial statements)', re.DOTALL | re.IGNORECASE)\n",
    "    management_discussion_pattern = re.compile(r'(?<=management discussion:)(.*?)(?=end of management discussion)', re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    financial_statements = financial_statement_pattern.search(document_text)\n",
    "    management_discussion = management_discussion_pattern.search(document_text)\n",
    "\n",
    "    return {\n",
    "        'financial_statements': financial_statements.group(1).strip() if financial_statements else None,\n",
    "        'management_discussion': management_discussion.group(1).strip() if management_discussion else None,\n",
    "    }\n",
    "\n",
    "def initialize_database(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('DROP TABLE IF EXISTS filings')\n",
    "    cursor.execute('''CREATE TABLE filings (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        company_name TEXT,\n",
    "                        filing_date TEXT,\n",
    "                        accession_number TEXT,\n",
    "                        form_type TEXT,\n",
    "                        business_address TEXT,\n",
    "                        document_type TEXT,\n",
    "                        document_text TEXT\n",
    "                    )''')\n",
    "\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS financial_statements (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        filing_id INTEGER,\n",
    "                        content TEXT,\n",
    "                        FOREIGN KEY (filing_id) REFERENCES filings(id)\n",
    "                    )''')\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS management_discussions (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        filing_id INTEGER,\n",
    "                        content TEXT,\n",
    "                        FOREIGN KEY (filing_id) REFERENCES filings(id)\n",
    "                    )''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database initialized with new schema.\")\n",
    "\n",
    "def save_to_database(data, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''INSERT INTO filings (company_name, filing_date, accession_number, form_type, business_address, document_type, document_text)\n",
    "                      VALUES (?, ?, ?, ?, ?, ?, ?)''',\n",
    "                   (data['company_name'], data['filing_date'], data['accession_number'], data['form_type'], data['business_address'], data['document_type'], data['document_text']))\n",
    "\n",
    "    filing_id = cursor.lastrowid\n",
    "\n",
    "    parsed_data = parse_document_text(data['document_text'])\n",
    "    parsed_data['filing_id'] = filing_id\n",
    "\n",
    "    if parsed_data['financial_statements']:\n",
    "        cursor.execute('''INSERT INTO financial_statements (filing_id, content)\n",
    "                          VALUES (?, ?)''',\n",
    "                       (filing_id, parsed_data['financial_statements']))\n",
    "    if parsed_data['management_discussion']:\n",
    "        cursor.execute('''INSERT INTO management_discussions (filing_id, content)\n",
    "                          VALUES (?, ?)''',\n",
    "                       (filing_id, parsed_data['management_discussion']))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_data_to_file(output_file, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('SELECT * FROM filings')\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for row in rows:\n",
    "            f.write(str(row) + '\\n')\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "def clean_text_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = re.sub(r'\\bfcx:\\w+\\b', '', line)\n",
    "        cleaned_line = re.sub(r'http://\\S+', '', cleaned_line)\n",
    "        \n",
    "        \n",
    "        cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip()\n",
    "        if cleaned_line:\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for cleaned_line in cleaned_lines:\n",
    "            file.write(cleaned_line + '\\n')\n",
    "\n",
    "\n",
    "def process_filings(filing_type, company_name, db_path, text_files_dir, output_file):\n",
    "    initialize_database(db_path)\n",
    "    \n",
    "    for filename in os.listdir(text_files_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(text_files_dir, filename)\n",
    "            data = extract_data(file_path)\n",
    "            save_to_database(data, db_path)\n",
    "    \n",
    "    save_data_to_file(output_file, db_path)\n",
    "    clean_text_file(output_file, f\"cleaned_{output_file}\")\n",
    "    print(f\"Data extraction and saving completed for {filing_type} filings of {company_name}.\")\n",
    "    print(f\"Data has been written to {output_file}.\")\n",
    "\n",
    "# List of companies and their tickers\n",
    "companies = [\n",
    "    {\"name\": \"fcx\", \"ticker\": \"FCX\"},\n",
    "    {\"name\": \"nem\", \"ticker\": \"NEM\"},\n",
    "    {\"name\": \"scco\", \"ticker\": \"SCCO\"},\n",
    "    {\"name\": \"aa\", \"ticker\": \"AA\"},\n",
    "    {\"name\": \"hl\", \"ticker\": \"HL\"}\n",
    "    \n",
    "]\n",
    "\n",
    "base_dir = r'C:\\Users\\avani\\Desktop\\Thesis\\mining'\n",
    "\n",
    "for company in companies:\n",
    "    for filing_type in [\"10-K\"]:\n",
    "        db_path = os.path.join(base_dir, company['name'], f'test3_{company[\"ticker\"]}_{filing_type}.db')\n",
    "        text_files_dir = os.path.join(base_dir, company['name'], filing_type)\n",
    "        output_file = f'{company[\"ticker\"]}_{filing_type}.txt'\n",
    "        \n",
    "        process_filings(filing_type, company['name'], db_path, text_files_dir, output_file)\n",
    "\n",
    "print(\"All filings processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2faafa51-81f4-4079-aed0-e5f11c4c1431",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-02 12:48:11,065 - INFO - Attempting to download FILING_10Q filings for ticker XOM (Attempt 1)\n",
      "310it [00:31,  9.95it/s]                                                                                               \n",
      "2024-07-02 12:48:45,045 - INFO - Successfully downloaded FILING_10Q filings for ticker XOM\n",
      "2024-07-02 12:48:46,053 - INFO - Attempting to download FILING_10K filings for ticker XOM (Attempt 1)\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 60/60 [00:06<00:00,  9.79it/s]\n",
      "2024-07-02 12:48:53,957 - INFO - Successfully downloaded FILING_10K filings for ticker XOM\n",
      "2024-07-02 12:48:54,966 - INFO - Attempting to download FILING_8K filings for ticker XOM (Attempt 1)\n",
      "1140it [01:55,  9.90it/s]                                                                                              \n",
      "2024-07-02 12:50:58,697 - INFO - Successfully downloaded FILING_8K filings for ticker XOM\n",
      "2024-07-02 12:50:58,697 - INFO - Attempting to download FILING_10Q filings for ticker CVX (Attempt 1)\n",
      "310it [00:31,  9.95it/s]                                                                                               \n",
      "2024-07-02 12:51:32,688 - INFO - Successfully downloaded FILING_10Q filings for ticker CVX\n",
      "2024-07-02 12:51:33,699 - INFO - Attempting to download FILING_10K filings for ticker CVX (Attempt 1)\n",
      "60it [00:08,  7.00it/s]                                                                                                \n",
      "2024-07-02 12:51:43,526 - INFO - Successfully downloaded FILING_10K filings for ticker CVX\n",
      "2024-07-02 12:51:44,535 - INFO - Attempting to download FILING_8K filings for ticker CVX (Attempt 1)\n",
      "1140it [01:54,  9.93it/s]                                                                                              \n",
      "2024-07-02 12:53:50,042 - INFO - Successfully downloaded FILING_8K filings for ticker CVX\n",
      "2024-07-02 12:53:50,044 - INFO - Attempting to download FILING_10Q filings for ticker COP (Attempt 1)\n",
      "210it [00:22,  9.48it/s]                                                                                               \n",
      "2024-07-02 12:54:14,565 - INFO - Successfully downloaded FILING_10Q filings for ticker COP\n",
      "2024-07-02 12:54:15,568 - INFO - Attempting to download FILING_10K filings for ticker COP (Attempt 1)\n",
      "40it [00:06,  5.81it/s]                                                                                                \n",
      "2024-07-02 12:54:23,500 - INFO - Successfully downloaded FILING_10K filings for ticker COP\n",
      "2024-07-02 12:54:24,507 - INFO - Attempting to download FILING_8K filings for ticker COP (Attempt 1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 920/920 [01:32<00:00,  9.96it/s]\n",
      "2024-07-02 12:56:04,247 - INFO - Successfully downloaded FILING_8K filings for ticker COP\n",
      "2024-07-02 12:56:04,247 - INFO - Attempting to download FILING_10Q filings for ticker OXY (Attempt 1)\n",
      "300it [00:30,  9.94it/s]                                                                                               \n",
      "2024-07-02 12:56:37,146 - INFO - Successfully downloaded FILING_10Q filings for ticker OXY\n",
      "2024-07-02 12:56:38,154 - INFO - Attempting to download FILING_10K filings for ticker OXY (Attempt 1)\n",
      "50it [00:05,  9.35it/s]                                                                                                \n",
      "2024-07-02 12:56:44,640 - INFO - Successfully downloaded FILING_10K filings for ticker OXY\n",
      "2024-07-02 12:56:45,649 - INFO - Attempting to download FILING_8K filings for ticker OXY (Attempt 1)\n",
      "1470it [02:27,  9.95it/s]                                                                                              \n",
      "2024-07-02 12:59:27,136 - INFO - Successfully downloaded FILING_8K filings for ticker OXY\n",
      "2024-07-02 12:59:27,136 - INFO - Attempting to download FILING_10Q filings for ticker SLB (Attempt 1)\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 280/280 [00:28<00:00,  9.95it/s]\n",
      "2024-07-02 12:59:57,400 - INFO - Successfully downloaded FILING_10Q filings for ticker SLB\n",
      "2024-07-02 12:59:58,409 - INFO - Attempting to download FILING_10K filings for ticker SLB (Attempt 1)\n",
      "40it [00:04,  8.75it/s]                                                                                                \n",
      "2024-07-02 13:00:03,908 - INFO - Successfully downloaded FILING_10K filings for ticker SLB\n",
      "2024-07-02 13:00:04,915 - INFO - Attempting to download FILING_8K filings for ticker SLB (Attempt 1)\n",
      "1070it [01:47,  9.96it/s]                                                                                              \n",
      "2024-07-02 13:02:00,239 - INFO - Successfully downloaded FILING_8K filings for ticker SLB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filings download process completed.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 8-K filings of xom.\n",
      "Data has been written to XOM_8-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-K filings of xom.\n",
      "Data has been written to XOM_10-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-Q filings of xom.\n",
      "Data has been written to XOM_10-Q.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 8-K filings of cvx.\n",
      "Data has been written to CVX_8-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-K filings of cvx.\n",
      "Data has been written to CVX_10-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-Q filings of cvx.\n",
      "Data has been written to CVX_10-Q.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 8-K filings of cop.\n",
      "Data has been written to COP_8-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-K filings of cop.\n",
      "Data has been written to COP_10-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-Q filings of cop.\n",
      "Data has been written to COP_10-Q.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 8-K filings of oxy.\n",
      "Data has been written to OXY_8-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-K filings of oxy.\n",
      "Data has been written to OXY_10-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-Q filings of oxy.\n",
      "Data has been written to OXY_10-Q.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 8-K filings of slb.\n",
      "Data has been written to SLB_8-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-K filings of slb.\n",
      "Data has been written to SLB_10-K.txt.\n",
      "Database initialized with new schema.\n",
      "Data extraction and saving completed for 10-Q filings of slb.\n",
      "Data has been written to SLB_10-Q.txt.\n",
      "All filings processed.\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import logging\n",
    "from secedgar import filings, FilingType\n",
    "from secedgar.exceptions import EDGARQueryError, NoFilingsError\n",
    "from time import sleep\n",
    "import os\n",
    "import re\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Apply the nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"filings_download.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define the tickers for the specified utility companies\n",
    "utility_tickers = [\n",
    "    \"XOM\",  \n",
    "    \"CVX\",  \n",
    "    \"COP\",  \n",
    "    \"OXY\",    \n",
    "    \"MPC\"    \n",
    "]\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 3\n",
    "\n",
    "# Function to download filings with retry logic and logging\n",
    "def download_filings(ticker, filing_type, base_path, user_agent):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to download {filing_type.name} filings for ticker {ticker} (Attempt {attempt + 1})\")\n",
    "            my_filings = filings(cik_lookup=[ticker],\n",
    "                                 filing_type=filing_type,\n",
    "                                 user_agent=user_agent)\n",
    "            my_filings.save(base_path)\n",
    "            logging.info(f\"Successfully downloaded {filing_type.name} filings for ticker {ticker}\")\n",
    "            break\n",
    "        except (EDGARQueryError, NoFilingsError) as e:\n",
    "            logging.error(f\"Error downloading {filing_type.name} filings for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "\n",
    "# Define the base path where the filings are saved\n",
    "base_path = r'C:\\Users\\avani\\Desktop\\Thesis\\oilandgas'\n",
    "user_agent = \"am1623@ic.ac.uk\"\n",
    "\n",
    "# Download filings for each company\n",
    "for ticker in utility_tickers:\n",
    "    download_filings(ticker, FilingType.FILING_10Q, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_10K, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_8K, base_path, user_agent)\n",
    "\n",
    "print(\"Filings download process completed.\")\n",
    "\n",
    "def extract_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "    company_name = re.search(r'COMPANY CONFORMED NAME:\\s*(.+)', content)\n",
    "    company_name = company_name.group(1) if company_name else None\n",
    "\n",
    "    filing_date = re.search(r'<ACCEPTANCE-DATETIME>(\\d+)', content)\n",
    "    filing_date = filing_date.group(1) if filing_date else None\n",
    "\n",
    "    accession_number = re.search(r'ACCESSION NUMBER:\\s*(.+)', content)\n",
    "    accession_number = accession_number.group(1) if accession_number else None\n",
    "\n",
    "    form_type = re.search(r'FORM TYPE:\\s*(.+)', content)\n",
    "    form_type = form_type.group(1) if form_type else None\n",
    "\n",
    "    business_address = re.search(r'BUSINESS ADDRESS:\\s+STREET 1:\\s+(.+)\\s+STREET 2:\\s+(.+)\\s+CITY:\\s+(.+)\\s+STATE:\\s+(.+)\\s+ZIP:\\s+(\\d+)', content)\n",
    "    business_address = f\"{business_address.group(1)}, {business_address.group(2)}, {business_address.group(3)}, {business_address.group(4)}, {business_address.group(5)}\" if business_address else None\n",
    "\n",
    "    document_type = re.search(r'<TYPE>(.+)', content)\n",
    "    document_type = document_type.group(1) if document_type else None\n",
    "\n",
    "    document_text = re.search(r'<TEXT>(.*?)</TEXT>', content, re.DOTALL)\n",
    "    document_text = document_text.group(1).strip() if document_text else None\n",
    "\n",
    "    if document_text:\n",
    "        document_text = clean_html(document_text)\n",
    "\n",
    "    return {\n",
    "        'company_name': company_name,\n",
    "        'filing_date': filing_date,\n",
    "        'accession_number': accession_number,\n",
    "        'form_type': form_type,\n",
    "        'business_address': business_address,\n",
    "        'document_type': document_type,\n",
    "        'document_text': document_text,\n",
    "    }\n",
    "\n",
    "def clean_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    text = soup.get_text(separator=' ', strip=True)\n",
    "    \n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Remove numeric patterns (e.g., numbers, dates, CIKs)\n",
    "    text = re.sub(r'\\b\\d{4,}\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d{1,2}\\b', '', text)\n",
    "    text = re.sub(r'\\b\\d+\\b', '', text)\n",
    "    text = re.sub(r'\\b[A-Za-z]{3}\\d{2}\\b', '', text)\n",
    "    # Remove unwanted characters and extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def parse_document_text(document_text):\n",
    "    financial_statement_pattern = re.compile(r'(?<=financial statements:)(.*?)(?=end of financial statements)', re.DOTALL | re.IGNORECASE)\n",
    "    management_discussion_pattern = re.compile(r'(?<=management discussion:)(.*?)(?=end of management discussion)', re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "    financial_statements = financial_statement_pattern.search(document_text)\n",
    "    management_discussion = management_discussion_pattern.search(document_text)\n",
    "\n",
    "    return {\n",
    "        'financial_statements': financial_statements.group(1).strip() if financial_statements else None,\n",
    "        'management_discussion': management_discussion.group(1).strip() if management_discussion else None,\n",
    "    }\n",
    "\n",
    "def initialize_database(db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('DROP TABLE IF EXISTS filings')\n",
    "    cursor.execute('''CREATE TABLE filings (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        company_name TEXT,\n",
    "                        filing_date TEXT,\n",
    "                        accession_number TEXT,\n",
    "                        form_type TEXT,\n",
    "                        business_address TEXT,\n",
    "                        document_type TEXT,\n",
    "                        document_text TEXT\n",
    "                    )''')\n",
    "\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS financial_statements (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        filing_id INTEGER,\n",
    "                        content TEXT,\n",
    "                        FOREIGN KEY (filing_id) REFERENCES filings(id)\n",
    "                    )''')\n",
    "    cursor.execute('''CREATE TABLE IF NOT EXISTS management_discussions (\n",
    "                        id INTEGER PRIMARY KEY,\n",
    "                        filing_id INTEGER,\n",
    "                        content TEXT,\n",
    "                        FOREIGN KEY (filing_id) REFERENCES filings(id)\n",
    "                    )''')\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    print(\"Database initialized with new schema.\")\n",
    "\n",
    "def save_to_database(data, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('''INSERT INTO filings (company_name, filing_date, accession_number, form_type, business_address, document_type, document_text)\n",
    "                      VALUES (?, ?, ?, ?, ?, ?, ?)''',\n",
    "                   (data['company_name'], data['filing_date'], data['accession_number'], data['form_type'], data['business_address'], data['document_type'], data['document_text']))\n",
    "\n",
    "    filing_id = cursor.lastrowid\n",
    "\n",
    "    parsed_data = parse_document_text(data['document_text'])\n",
    "    parsed_data['filing_id'] = filing_id\n",
    "\n",
    "    if parsed_data['financial_statements']:\n",
    "        cursor.execute('''INSERT INTO financial_statements (filing_id, content)\n",
    "                          VALUES (?, ?)''',\n",
    "                       (filing_id, parsed_data['financial_statements']))\n",
    "    if parsed_data['management_discussion']:\n",
    "        cursor.execute('''INSERT INTO management_discussions (filing_id, content)\n",
    "                          VALUES (?, ?)''',\n",
    "                       (filing_id, parsed_data['management_discussion']))\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def save_data_to_file(output_file, db_path):\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cursor = conn.cursor()\n",
    "\n",
    "    cursor.execute('SELECT * FROM filings')\n",
    "    rows = cursor.fetchall()\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for row in rows:\n",
    "            f.write(str(row) + '\\n')\n",
    "\n",
    "    conn.close()\n",
    "\n",
    "def clean_text_file(input_file, output_file):\n",
    "    with open(input_file, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    cleaned_lines = []\n",
    "    for line in lines:\n",
    "        cleaned_line = re.sub(r'\\bfcx:\\w+\\b', '', line)\n",
    "        cleaned_line = re.sub(r'http://\\S+', '', cleaned_line)\n",
    "        cleaned_line = re.sub(r'\\b\\d+\\b', '', cleaned_line)\n",
    "        cleaned_line = re.sub(r'\\s+', ' ', cleaned_line).strip()\n",
    "        if cleaned_line:\n",
    "            cleaned_lines.append(cleaned_line)\n",
    "\n",
    "    with open(output_file, 'w', encoding='utf-8') as file:\n",
    "        for cleaned_line in cleaned_lines:\n",
    "            file.write(cleaned_line + '\\n')\n",
    "\n",
    "def process_filings(filing_type, company_name, db_path, text_files_dir, output_file):\n",
    "    initialize_database(db_path)\n",
    "    \n",
    "    for filename in os.listdir(text_files_dir):\n",
    "        if filename.endswith('.txt'):\n",
    "            file_path = os.path.join(text_files_dir, filename)\n",
    "            data = extract_data(file_path)\n",
    "            save_to_database(data, db_path)\n",
    "    \n",
    "    save_data_to_file(output_file, db_path)\n",
    "    clean_text_file(output_file, f\"cleaned_{output_file}\")\n",
    "    print(f\"Data extraction and saving completed for {filing_type} filings of {company_name}.\")\n",
    "    print(f\"Data has been written to {output_file}.\")\n",
    "\n",
    "# List of companies and their tickers\n",
    "companies = [\n",
    "    {\"name\": \"xom\", \"ticker\": \"XOM\"},\n",
    "    {\"name\": \"cvx\", \"ticker\": \"CVX\"},\n",
    "    {\"name\": \"cop\", \"ticker\": \"COP\"},\n",
    "    {\"name\": \"oxy\", \"ticker\": \"OXY\"},\n",
    "    {\"name\": \"mpc\", \"ticker\": \"MPC\"}\n",
    "]\n",
    "\n",
    "base_dir = r'C:\\Users\\avani\\Desktop\\Thesis\\oilandgas'\n",
    "\n",
    "for company in companies:\n",
    "    for filing_type in [\"10-K\"]:\n",
    "        db_path = os.path.join(base_dir, company['name'], f'test3_{company[\"ticker\"]}_{filing_type}.db')\n",
    "        text_files_dir = os.path.join(base_dir, company['name'], filing_type)\n",
    "        output_file = f'{company[\"ticker\"]}_{filing_type}.txt'\n",
    "        \n",
    "        process_filings(filing_type, company['name'], db_path, text_files_dir, output_file)\n",
    "\n",
    "print(\"All filings processed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f9d4274-4359-48e3-8a07-7aca498c5539",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "import logging\n",
    "from secedgar import filings, FilingType\n",
    "from secedgar.exceptions import EDGARQueryError, NoFilingsError\n",
    "from time import sleep\n",
    "import os\n",
    "\n",
    "# Apply the nest_asyncio to allow nested event loops\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Configure logging to log to both console and file\n",
    "logging.basicConfig(level=logging.INFO, \n",
    "                    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "                    handlers=[\n",
    "                        logging.FileHandler(\"filings_download.log\"),\n",
    "                        logging.StreamHandler()\n",
    "                    ])\n",
    "\n",
    "# Define the tickers for the specified utility companies\n",
    "utility_tickers = [\n",
    "    \"NEE\",  # NextEra Energy\n",
    "    \"DUK\",  # Duke Energy\n",
    "    \"EXC\",  # Exelon Corporation\n",
    "    \"D\",    # Dominion Energy\n",
    "    \"ED\"    # Consolidated Edison\n",
    "]\n",
    "\n",
    "# Define the maximum number of retries\n",
    "max_retries = 3\n",
    "\n",
    "# Function to download filings with retry logic and logging\n",
    "def download_filings(ticker, filing_type, base_path, user_agent):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            logging.info(f\"Attempting to download {filing_type.name} filings for ticker {ticker} (Attempt {attempt + 1})\")\n",
    "            my_filings = filings(cik_lookup=[ticker],\n",
    "                                 filing_type=filing_type,\n",
    "                                 user_agent=user_agent)\n",
    "            my_filings.save(base_path)\n",
    "            logging.info(f\"Successfully downloaded {filing_type.name} filings for ticker {ticker}\")\n",
    "            break\n",
    "        except (EDGARQueryError, NoFilingsError) as e:\n",
    "            logging.error(f\"Error downloading {filing_type.name} filings for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Unexpected error for ticker {ticker}: {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                logging.info(f\"Retrying in 5 seconds...\")\n",
    "                sleep(5)\n",
    "            else:\n",
    "                logging.error(f\"Failed to download {filing_type.name} filings for ticker {ticker} after {max_retries} attempts\")\n",
    "\n",
    "# Define the base path where the filings are saved\n",
    "base_path = r'C:\\Users\\avani\\Desktop\\Thesis\\utilities'\n",
    "user_agent = \"am1623@ic.ac.uk\"\n",
    "\n",
    "# Download filings for each company\n",
    "for ticker in utility_tickers:\n",
    "    download_filings(ticker, FilingType.FILING_10Q, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_10K, base_path, user_agent)\n",
    "    sleep(1)  # Pause between different filings\n",
    "    download_filings(ticker, FilingType.FILING_8K, base_path, user_agent)\n",
    "\n",
    "print(\"Filings download process completed.\")\n",
    "\n",
    "# List of companies and their tickers\n",
    "companies = [\n",
    "    {\"name\": \"d\", \"ticker\": \"D\"},\n",
    "    {\"name\": \"nee\", \"ticker\": \"NEE\"},\n",
    "    {\"name\": \"duk\", \"ticker\": \"DUK\"},\n",
    "    {\"name\": \"exc\", \"ticker\": \"EXC\"},\n",
    "    {\"name\": \"ed\", \"ticker\": \"ED\"}\n",
    "]\n",
    "\n",
    "base_dir = r'C:\\Users\\avani\\Desktop\\Thesis\\utilities'\n",
    "\n",
    "for company in companies:\n",
    "    for filing_type in [\"8-K\", \"10-K\", \"10-Q\"]:\n",
    "        db_path = os.path.join(base_dir, company['name'], f'test3_{company[\"ticker\"]}_{filing_type}.db')\n",
    "        text_files_dir = os.path.join(base_dir, company['name'], filing_type)\n",
    "        output_file = f'{company[\"ticker\"]}_{filing_type}.txt'\n",
    "        \n",
    "        process_filings(filing_type, company['name'], db_path, text_files_dir, output_file)\n",
    "\n",
    "print(\"All filings processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
